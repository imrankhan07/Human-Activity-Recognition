{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test and train set\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "0         -0.923527         -0.934724  ...                        -0.710304   \n",
       "1         -0.957686         -0.943068  ...                        -0.861499   \n",
       "2         -0.977469         -0.938692  ...                        -0.760104   \n",
       "3         -0.989302         -0.938692  ...                        -0.482845   \n",
       "4         -0.990441         -0.942469  ...                        -0.699205   \n",
       "\n",
       "   angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "0                    -0.112754                              0.030400   \n",
       "1                     0.053477                             -0.007435   \n",
       "2                    -0.118559                              0.177899   \n",
       "3                    -0.036788                             -0.012892   \n",
       "4                     0.123320                              0.122542   \n",
       "\n",
       "   angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "0                         -0.464761                             -0.018446   \n",
       "1                         -0.732626                              0.703511   \n",
       "2                          0.100699                              0.808529   \n",
       "3                          0.640011                             -0.485366   \n",
       "4                          0.693578                             -0.615971   \n",
       "\n",
       "   angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  subject  \\\n",
       "0             -0.841247              0.179941             -0.058627        1   \n",
       "1             -0.844788              0.180289             -0.054317        1   \n",
       "2             -0.848933              0.180637             -0.049118        1   \n",
       "3             -0.848649              0.181935             -0.047663        1   \n",
       "4             -0.847865              0.185151             -0.043892        1   \n",
       "\n",
       "   Activity  \n",
       "0  STANDING  \n",
       "1  STANDING  \n",
       "2  STANDING  \n",
       "3  STANDING  \n",
       "4  STANDING  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WALKING               209\n",
       "STANDING              179\n",
       "LAYING                164\n",
       "WALKING_UPSTAIRS      159\n",
       "WALKING_DOWNSTAIRS    145\n",
       "SITTING               143\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((999, 563), (999, 563))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WALKING               185\n",
       "LAYING                183\n",
       "STANDING              178\n",
       "SITTING               170\n",
       "WALKING_UPSTAIRS      149\n",
       "WALKING_DOWNSTAIRS    134\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z',\n",
       "       'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z',\n",
       "       'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z',\n",
       "       'tBodyAcc-max()-X',\n",
       "       ...\n",
       "       'fBodyBodyGyroJerkMag-kurtosis()', 'angle(tBodyAccMean,gravity)',\n",
       "       'angle(tBodyAccJerkMean),gravityMean)',\n",
       "       'angle(tBodyGyroMean,gravityMean)',\n",
       "       'angle(tBodyGyroJerkMean,gravityMean)', 'angle(X,gravityMean)',\n",
       "       'angle(Y,gravityMean)', 'angle(Z,gravityMean)', 'subject', 'Activity'],\n",
       "      dtype='object', length=563)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.272522</td>\n",
       "      <td>-0.017315</td>\n",
       "      <td>-0.106699</td>\n",
       "      <td>-0.564767</td>\n",
       "      <td>-0.421911</td>\n",
       "      <td>-0.601705</td>\n",
       "      <td>-0.596136</td>\n",
       "      <td>-0.439798</td>\n",
       "      <td>-0.591796</td>\n",
       "      <td>-0.411679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219581</td>\n",
       "      <td>-0.541213</td>\n",
       "      <td>0.015125</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>-0.013146</td>\n",
       "      <td>-0.545935</td>\n",
       "      <td>0.058899</td>\n",
       "      <td>-0.033470</td>\n",
       "      <td>2.936937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.070183</td>\n",
       "      <td>0.041918</td>\n",
       "      <td>0.056029</td>\n",
       "      <td>0.428018</td>\n",
       "      <td>0.501715</td>\n",
       "      <td>0.340713</td>\n",
       "      <td>0.400180</td>\n",
       "      <td>0.486153</td>\n",
       "      <td>0.351317</td>\n",
       "      <td>0.530312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351241</td>\n",
       "      <td>0.360838</td>\n",
       "      <td>0.349059</td>\n",
       "      <td>0.469766</td>\n",
       "      <td>0.629273</td>\n",
       "      <td>0.481251</td>\n",
       "      <td>0.471809</td>\n",
       "      <td>0.349956</td>\n",
       "      <td>0.168279</td>\n",
       "      <td>1.636577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.361205</td>\n",
       "      <td>-0.684097</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999300</td>\n",
       "      <td>-0.998359</td>\n",
       "      <td>-0.999454</td>\n",
       "      <td>-0.999407</td>\n",
       "      <td>-0.998077</td>\n",
       "      <td>-0.999808</td>\n",
       "      <td>-0.971348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.944282</td>\n",
       "      <td>-0.999595</td>\n",
       "      <td>-0.939598</td>\n",
       "      <td>-0.976454</td>\n",
       "      <td>-0.995222</td>\n",
       "      <td>-0.994877</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.875487</td>\n",
       "      <td>-0.980143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.258468</td>\n",
       "      <td>-0.025925</td>\n",
       "      <td>-0.122726</td>\n",
       "      <td>-0.990822</td>\n",
       "      <td>-0.968894</td>\n",
       "      <td>-0.973927</td>\n",
       "      <td>-0.992207</td>\n",
       "      <td>-0.970928</td>\n",
       "      <td>-0.972885</td>\n",
       "      <td>-0.933391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.476273</td>\n",
       "      <td>-0.815733</td>\n",
       "      <td>-0.144342</td>\n",
       "      <td>-0.316220</td>\n",
       "      <td>-0.512150</td>\n",
       "      <td>-0.392773</td>\n",
       "      <td>-0.795959</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>-0.103119</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277054</td>\n",
       "      <td>-0.017185</td>\n",
       "      <td>-0.108829</td>\n",
       "      <td>-0.464909</td>\n",
       "      <td>-0.208523</td>\n",
       "      <td>-0.486962</td>\n",
       "      <td>-0.509215</td>\n",
       "      <td>-0.242165</td>\n",
       "      <td>-0.465163</td>\n",
       "      <td>-0.297181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.264434</td>\n",
       "      <td>-0.629299</td>\n",
       "      <td>0.010903</td>\n",
       "      <td>0.017954</td>\n",
       "      <td>0.012891</td>\n",
       "      <td>-0.016025</td>\n",
       "      <td>-0.717300</td>\n",
       "      <td>0.223164</td>\n",
       "      <td>0.030593</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.290635</td>\n",
       "      <td>-0.007523</td>\n",
       "      <td>-0.093717</td>\n",
       "      <td>-0.234604</td>\n",
       "      <td>0.045610</td>\n",
       "      <td>-0.314822</td>\n",
       "      <td>-0.289766</td>\n",
       "      <td>0.012828</td>\n",
       "      <td>-0.302503</td>\n",
       "      <td>-0.008596</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015262</td>\n",
       "      <td>-0.361251</td>\n",
       "      <td>0.178357</td>\n",
       "      <td>0.332586</td>\n",
       "      <td>0.538985</td>\n",
       "      <td>0.350713</td>\n",
       "      <td>-0.606748</td>\n",
       "      <td>0.281283</td>\n",
       "      <td>0.082679</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.498177</td>\n",
       "      <td>0.324130</td>\n",
       "      <td>0.346658</td>\n",
       "      <td>0.543347</td>\n",
       "      <td>0.532506</td>\n",
       "      <td>0.364114</td>\n",
       "      <td>0.495926</td>\n",
       "      <td>0.502260</td>\n",
       "      <td>0.554965</td>\n",
       "      <td>0.680338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>0.955207</td>\n",
       "      <td>0.998425</td>\n",
       "      <td>0.994519</td>\n",
       "      <td>0.971511</td>\n",
       "      <td>0.799174</td>\n",
       "      <td>0.385117</td>\n",
       "      <td>0.265795</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
       "count         999.000000         999.000000         999.000000   \n",
       "mean            0.272522          -0.017315          -0.106699   \n",
       "std             0.070183           0.041918           0.056029   \n",
       "min            -0.361205          -0.684097          -1.000000   \n",
       "25%             0.258468          -0.025925          -0.122726   \n",
       "50%             0.277054          -0.017185          -0.108829   \n",
       "75%             0.290635          -0.007523          -0.093717   \n",
       "max             0.498177           0.324130           0.346658   \n",
       "\n",
       "       tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
       "count        999.000000        999.000000        999.000000        999.000000   \n",
       "mean          -0.564767         -0.421911         -0.601705         -0.596136   \n",
       "std            0.428018          0.501715          0.340713          0.400180   \n",
       "min           -0.999300         -0.998359         -0.999454         -0.999407   \n",
       "25%           -0.990822         -0.968894         -0.973927         -0.992207   \n",
       "50%           -0.464909         -0.208523         -0.486962         -0.509215   \n",
       "75%           -0.234604          0.045610         -0.314822         -0.289766   \n",
       "max            0.543347          0.532506          0.364114          0.495926   \n",
       "\n",
       "       tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  \\\n",
       "count        999.000000        999.000000        999.000000  ...   \n",
       "mean          -0.439798         -0.591796         -0.411679  ...   \n",
       "std            0.486153          0.351317          0.530312  ...   \n",
       "min           -0.998077         -0.999808         -0.971348  ...   \n",
       "25%           -0.970928         -0.972885         -0.933391  ...   \n",
       "50%           -0.242165         -0.465163         -0.297181  ...   \n",
       "75%            0.012828         -0.302503         -0.008596  ...   \n",
       "max            0.502260          0.554965          0.680338  ...   \n",
       "\n",
       "       fBodyBodyGyroJerkMag-skewness()  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "count                       999.000000                       999.000000   \n",
       "mean                         -0.219581                        -0.541213   \n",
       "std                           0.351241                         0.360838   \n",
       "min                          -0.944282                        -0.999595   \n",
       "25%                          -0.476273                        -0.815733   \n",
       "50%                          -0.264434                        -0.629299   \n",
       "75%                          -0.015262                        -0.361251   \n",
       "max                           0.989538                         0.956845   \n",
       "\n",
       "       angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "count                   999.000000                            999.000000   \n",
       "mean                      0.015125                              0.002728   \n",
       "std                       0.349059                              0.469766   \n",
       "min                      -0.939598                             -0.976454   \n",
       "25%                      -0.144342                             -0.316220   \n",
       "50%                       0.010903                              0.017954   \n",
       "75%                       0.178357                              0.332586   \n",
       "max                       0.955207                              0.998425   \n",
       "\n",
       "       angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "count                        999.000000                            999.000000   \n",
       "mean                           0.004282                             -0.013146   \n",
       "std                            0.629273                              0.481251   \n",
       "min                           -0.995222                             -0.994877   \n",
       "25%                           -0.512150                             -0.392773   \n",
       "50%                            0.012891                             -0.016025   \n",
       "75%                            0.538985                              0.350713   \n",
       "max                            0.994519                              0.971511   \n",
       "\n",
       "       angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  \\\n",
       "count            999.000000            999.000000            999.000000   \n",
       "mean              -0.545935              0.058899             -0.033470   \n",
       "std                0.471809              0.349956              0.168279   \n",
       "min               -1.000000             -0.875487             -0.980143   \n",
       "25%               -0.795959              0.030145             -0.103119   \n",
       "50%               -0.717300              0.223164              0.030593   \n",
       "75%               -0.606748              0.281283              0.082679   \n",
       "max                0.799174              0.385117              0.265795   \n",
       "\n",
       "          subject  \n",
       "count  999.000000  \n",
       "mean     2.936937  \n",
       "std      1.636577  \n",
       "min      1.000000  \n",
       "25%      1.000000  \n",
       "50%      3.000000  \n",
       "75%      5.000000  \n",
       "max      6.000000  \n",
       "\n",
       "[8 rows x 562 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling data\n",
    "from sklearn.utils import shuffle\n",
    "train= shuffle(train)\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating features and labels\n",
    "trainData= train.drop('Activity',axis=1).values\n",
    "trainLabel= train.Activity.values\n",
    "\n",
    "testData= test.drop('Activity', axis=1).values\n",
    "testLabel= test.Activity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding labels\n",
    "from sklearn import preprocessing\n",
    "\n",
    "encoder= preprocessing.LabelEncoder()\n",
    "\n",
    "#encoding test labels\n",
    "encoder.fit(testLabel)\n",
    "testLabelE = encoder.transform(testLabel)\n",
    "\n",
    "#encoding train labels\n",
    "encoder.fit(trainLabel)\n",
    "trainLabelE = encoder.transform(trainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification models:\n",
    "#NN using MLP\n",
    "#Logistic regression\n",
    "#Random forest classifier\n",
    "#KNN\n",
    "#Decision Tree\n",
    "#Grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying supervised neural network using multi-layer-preceptron\n",
    "import sklearn.neural_network as nn\n",
    "mlpSGD  =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
    "                        , max_iter=1000 , alpha=1e-4  \\\n",
    "                        , solver='sgd' , verbose=10   \\\n",
    "                        , tol=1e-19 , random_state=1  \\\n",
    "                        , learning_rate_init=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpADAM  =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
    "                        , max_iter=1000 , alpha=1e-4  \\\n",
    "                        , solver='adam' , verbose=10   \\\n",
    "                        , tol=1e-19 , random_state=1  \\\n",
    "                        , learning_rate_init=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpLBFGS  =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
    "                        , max_iter=1000 , alpha=1e-4  \\\n",
    "                        , solver='lbfgs' , verbose=10   \\\n",
    "                        , tol=1e-19 , random_state=1  \\\n",
    "                        , learning_rate_init=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.15815749\n",
      "Iteration 2, loss = 1.88024477\n",
      "Iteration 3, loss = 1.66945500\n",
      "Iteration 4, loss = 1.51416605\n",
      "Iteration 5, loss = 1.37388403\n",
      "Iteration 6, loss = 1.27878935\n",
      "Iteration 7, loss = 1.20767962\n",
      "Iteration 8, loss = 1.14553221\n",
      "Iteration 9, loss = 1.08903384\n",
      "Iteration 10, loss = 1.04077282\n",
      "Iteration 11, loss = 0.99784058\n",
      "Iteration 12, loss = 0.95924068\n",
      "Iteration 13, loss = 0.92495456\n",
      "Iteration 14, loss = 0.89311561\n",
      "Iteration 15, loss = 0.86463003\n",
      "Iteration 16, loss = 0.83795070\n",
      "Iteration 17, loss = 0.81286112\n",
      "Iteration 18, loss = 0.78966031\n",
      "Iteration 19, loss = 0.76812018\n",
      "Iteration 20, loss = 0.74809077\n",
      "Iteration 21, loss = 0.72839528\n",
      "Iteration 22, loss = 0.71034400\n",
      "Iteration 23, loss = 0.69362279\n",
      "Iteration 24, loss = 0.67688873\n",
      "Iteration 25, loss = 0.66134716\n",
      "Iteration 26, loss = 0.64678325\n",
      "Iteration 27, loss = 0.63274531\n",
      "Iteration 28, loss = 0.61942609\n",
      "Iteration 29, loss = 0.60636496\n",
      "Iteration 30, loss = 0.59359270\n",
      "Iteration 31, loss = 0.58159681\n",
      "Iteration 32, loss = 0.57026394\n",
      "Iteration 33, loss = 0.55979767\n",
      "Iteration 34, loss = 0.54881789\n",
      "Iteration 35, loss = 0.53876255\n",
      "Iteration 36, loss = 0.52837006\n",
      "Iteration 37, loss = 0.51915534\n",
      "Iteration 38, loss = 0.50963688\n",
      "Iteration 39, loss = 0.50066596\n",
      "Iteration 40, loss = 0.49263172\n",
      "Iteration 41, loss = 0.48425414\n",
      "Iteration 42, loss = 0.47574873\n",
      "Iteration 43, loss = 0.46796864\n",
      "Iteration 44, loss = 0.46062786\n",
      "Iteration 45, loss = 0.45323183\n",
      "Iteration 46, loss = 0.44628895\n",
      "Iteration 47, loss = 0.43945492\n",
      "Iteration 48, loss = 0.43296148\n",
      "Iteration 49, loss = 0.42663175\n",
      "Iteration 50, loss = 0.42015330\n",
      "Iteration 51, loss = 0.41374452\n",
      "Iteration 52, loss = 0.40799001\n",
      "Iteration 53, loss = 0.40260969\n",
      "Iteration 54, loss = 0.39679725\n",
      "Iteration 55, loss = 0.39140404\n",
      "Iteration 56, loss = 0.38575998\n",
      "Iteration 57, loss = 0.38123003\n",
      "Iteration 58, loss = 0.37641963\n",
      "Iteration 59, loss = 0.37064200\n",
      "Iteration 60, loss = 0.36614699\n",
      "Iteration 61, loss = 0.36144867\n",
      "Iteration 62, loss = 0.35719304\n",
      "Iteration 63, loss = 0.35237475\n",
      "Iteration 64, loss = 0.34864949\n",
      "Iteration 65, loss = 0.34402000\n",
      "Iteration 66, loss = 0.33961370\n",
      "Iteration 67, loss = 0.33617289\n",
      "Iteration 68, loss = 0.33200806\n",
      "Iteration 69, loss = 0.32794497\n",
      "Iteration 70, loss = 0.32440380\n",
      "Iteration 71, loss = 0.32034001\n",
      "Iteration 72, loss = 0.31680683\n",
      "Iteration 73, loss = 0.31341673\n",
      "Iteration 74, loss = 0.30985328\n",
      "Iteration 75, loss = 0.30670587\n",
      "Iteration 76, loss = 0.30331400\n",
      "Iteration 77, loss = 0.29993601\n",
      "Iteration 78, loss = 0.29668457\n",
      "Iteration 79, loss = 0.29373147\n",
      "Iteration 80, loss = 0.29066592\n",
      "Iteration 81, loss = 0.28806946\n",
      "Iteration 82, loss = 0.28482764\n",
      "Iteration 83, loss = 0.28208885\n",
      "Iteration 84, loss = 0.27913895\n",
      "Iteration 85, loss = 0.27622015\n",
      "Iteration 86, loss = 0.27366928\n",
      "Iteration 87, loss = 0.27122877\n",
      "Iteration 88, loss = 0.26880709\n",
      "Iteration 89, loss = 0.26637751\n",
      "Iteration 90, loss = 0.26312956\n",
      "Iteration 91, loss = 0.26060599\n",
      "Iteration 92, loss = 0.25846818\n",
      "Iteration 93, loss = 0.25576133\n",
      "Iteration 94, loss = 0.25370398\n",
      "Iteration 95, loss = 0.25129580\n",
      "Iteration 96, loss = 0.24972004\n",
      "Iteration 97, loss = 0.24688785\n",
      "Iteration 98, loss = 0.24466454\n",
      "Iteration 99, loss = 0.24264537\n",
      "Iteration 100, loss = 0.24034480\n",
      "Iteration 101, loss = 0.23829214\n",
      "Iteration 102, loss = 0.23613494\n",
      "Iteration 103, loss = 0.23396496\n",
      "Iteration 104, loss = 0.23240881\n",
      "Iteration 105, loss = 0.23026371\n",
      "Iteration 106, loss = 0.22805969\n",
      "Iteration 107, loss = 0.22683244\n",
      "Iteration 108, loss = 0.22437249\n",
      "Iteration 109, loss = 0.22265727\n",
      "Iteration 110, loss = 0.22097080\n",
      "Iteration 111, loss = 0.21907357\n",
      "Iteration 112, loss = 0.21754296\n",
      "Iteration 113, loss = 0.21550293\n",
      "Iteration 114, loss = 0.21368406\n",
      "Iteration 115, loss = 0.21210870\n",
      "Iteration 116, loss = 0.21042839\n",
      "Iteration 117, loss = 0.20916587\n",
      "Iteration 118, loss = 0.20756278\n",
      "Iteration 119, loss = 0.20549913\n",
      "Iteration 120, loss = 0.20430180\n",
      "Iteration 121, loss = 0.20237805\n",
      "Iteration 122, loss = 0.20126261\n",
      "Iteration 123, loss = 0.19951765\n",
      "Iteration 124, loss = 0.19819357\n",
      "Iteration 125, loss = 0.19640654\n",
      "Iteration 126, loss = 0.19509381\n",
      "Iteration 127, loss = 0.19373803\n",
      "Iteration 128, loss = 0.19230089\n",
      "Iteration 129, loss = 0.19090341\n",
      "Iteration 130, loss = 0.18946457\n",
      "Iteration 131, loss = 0.18813645\n",
      "Iteration 132, loss = 0.18664018\n",
      "Iteration 133, loss = 0.18536954\n",
      "Iteration 134, loss = 0.18432583\n",
      "Iteration 135, loss = 0.18285056\n",
      "Iteration 136, loss = 0.18195147\n",
      "Iteration 137, loss = 0.18044381\n",
      "Iteration 138, loss = 0.17904033\n",
      "Iteration 139, loss = 0.17804541\n",
      "Iteration 140, loss = 0.17753916\n",
      "Iteration 141, loss = 0.17541826\n",
      "Iteration 142, loss = 0.17447579\n",
      "Iteration 143, loss = 0.17322664\n",
      "Iteration 144, loss = 0.17206726\n",
      "Iteration 145, loss = 0.17106305\n",
      "Iteration 146, loss = 0.16992594\n",
      "Iteration 147, loss = 0.16881244\n",
      "Iteration 148, loss = 0.16786489\n",
      "Iteration 149, loss = 0.16666859\n",
      "Iteration 150, loss = 0.16571795\n",
      "Iteration 151, loss = 0.16446601\n",
      "Iteration 152, loss = 0.16351524\n",
      "Iteration 153, loss = 0.16276197\n",
      "Iteration 154, loss = 0.16269901\n",
      "Iteration 155, loss = 0.16128407\n",
      "Iteration 156, loss = 0.15974854\n",
      "Iteration 157, loss = 0.15846457\n",
      "Iteration 158, loss = 0.15760175\n",
      "Iteration 159, loss = 0.15664999\n",
      "Iteration 160, loss = 0.15571548\n",
      "Iteration 161, loss = 0.15476136\n",
      "Iteration 162, loss = 0.15403656\n",
      "Iteration 163, loss = 0.15293961\n",
      "Iteration 164, loss = 0.15259464\n",
      "Iteration 165, loss = 0.15111460\n",
      "Iteration 166, loss = 0.15048475\n",
      "Iteration 167, loss = 0.14952454\n",
      "Iteration 168, loss = 0.14858823\n",
      "Iteration 169, loss = 0.14779199\n",
      "Iteration 170, loss = 0.14682954\n",
      "Iteration 171, loss = 0.14624198\n",
      "Iteration 172, loss = 0.14553123\n",
      "Iteration 173, loss = 0.14435867\n",
      "Iteration 174, loss = 0.14364337\n",
      "Iteration 175, loss = 0.14290491\n",
      "Iteration 176, loss = 0.14194245\n",
      "Iteration 177, loss = 0.14122547\n",
      "Iteration 178, loss = 0.14038527\n",
      "Iteration 179, loss = 0.13990956\n",
      "Iteration 180, loss = 0.13880675\n",
      "Iteration 181, loss = 0.13848675\n",
      "Iteration 182, loss = 0.13818623\n",
      "Iteration 183, loss = 0.13692531\n",
      "Iteration 184, loss = 0.13617489\n",
      "Iteration 185, loss = 0.13537193\n",
      "Iteration 186, loss = 0.13460258\n",
      "Iteration 187, loss = 0.13407964\n",
      "Iteration 188, loss = 0.13323523\n",
      "Iteration 189, loss = 0.13240412\n",
      "Iteration 190, loss = 0.13173493\n",
      "Iteration 191, loss = 0.13113348\n",
      "Iteration 192, loss = 0.13046297\n",
      "Iteration 193, loss = 0.12989985\n",
      "Iteration 194, loss = 0.12941859\n",
      "Iteration 195, loss = 0.12868697\n",
      "Iteration 196, loss = 0.12802316\n",
      "Iteration 197, loss = 0.12722224\n",
      "Iteration 198, loss = 0.12665749\n",
      "Iteration 199, loss = 0.12642324\n",
      "Iteration 200, loss = 0.12528525\n",
      "Iteration 201, loss = 0.12458762\n",
      "Iteration 202, loss = 0.12398231\n",
      "Iteration 203, loss = 0.12340903\n",
      "Iteration 204, loss = 0.12283325\n",
      "Iteration 205, loss = 0.12215119\n",
      "Iteration 206, loss = 0.12154844\n",
      "Iteration 207, loss = 0.12092472\n",
      "Iteration 208, loss = 0.12075271\n",
      "Iteration 209, loss = 0.11987846\n",
      "Iteration 210, loss = 0.11943156\n",
      "Iteration 211, loss = 0.11871599\n",
      "Iteration 212, loss = 0.11807362\n",
      "Iteration 213, loss = 0.11751072\n",
      "Iteration 214, loss = 0.11729647\n",
      "Iteration 215, loss = 0.11709559\n",
      "Iteration 216, loss = 0.11599962\n",
      "Iteration 217, loss = 0.11554530\n",
      "Iteration 218, loss = 0.11475873\n",
      "Iteration 219, loss = 0.11424110\n",
      "Iteration 220, loss = 0.11402006\n",
      "Iteration 221, loss = 0.11326950\n",
      "Iteration 222, loss = 0.11277322\n",
      "Iteration 223, loss = 0.11230034\n",
      "Iteration 224, loss = 0.11173221\n",
      "Iteration 225, loss = 0.11134642\n",
      "Iteration 226, loss = 0.11135689\n",
      "Iteration 227, loss = 0.11029376\n",
      "Iteration 228, loss = 0.10978883\n",
      "Iteration 229, loss = 0.10940702\n",
      "Iteration 230, loss = 0.10878587\n",
      "Iteration 231, loss = 0.10842269\n",
      "Iteration 232, loss = 0.10776600\n",
      "Iteration 233, loss = 0.10753939\n",
      "Iteration 234, loss = 0.10698899\n",
      "Iteration 235, loss = 0.10655657\n",
      "Iteration 236, loss = 0.10584655\n",
      "Iteration 237, loss = 0.10546829\n",
      "Iteration 238, loss = 0.10511170\n",
      "Iteration 239, loss = 0.10453367\n",
      "Iteration 240, loss = 0.10415952\n",
      "Iteration 241, loss = 0.10375878\n",
      "Iteration 242, loss = 0.10334221\n",
      "Iteration 243, loss = 0.10269446\n",
      "Iteration 244, loss = 0.10229375\n",
      "Iteration 245, loss = 0.10200152\n",
      "Iteration 246, loss = 0.10161582\n",
      "Iteration 247, loss = 0.10104474\n",
      "Iteration 248, loss = 0.10094955\n",
      "Iteration 249, loss = 0.10025457\n",
      "Iteration 250, loss = 0.09978912\n",
      "Iteration 251, loss = 0.09957296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.09902689\n",
      "Iteration 253, loss = 0.09870037\n",
      "Iteration 254, loss = 0.09820855\n",
      "Iteration 255, loss = 0.09785060\n",
      "Iteration 256, loss = 0.09739627\n",
      "Iteration 257, loss = 0.09695949\n",
      "Iteration 258, loss = 0.09667370\n",
      "Iteration 259, loss = 0.09618370\n",
      "Iteration 260, loss = 0.09609286\n",
      "Iteration 261, loss = 0.09550702\n",
      "Iteration 262, loss = 0.09507029\n",
      "Iteration 263, loss = 0.09489314\n",
      "Iteration 264, loss = 0.09448389\n",
      "Iteration 265, loss = 0.09404107\n",
      "Iteration 266, loss = 0.09357278\n",
      "Iteration 267, loss = 0.09314887\n",
      "Iteration 268, loss = 0.09294304\n",
      "Iteration 269, loss = 0.09234800\n",
      "Iteration 270, loss = 0.09213959\n",
      "Iteration 271, loss = 0.09167408\n",
      "Iteration 272, loss = 0.09149573\n",
      "Iteration 273, loss = 0.09098592\n",
      "Iteration 274, loss = 0.09057287\n",
      "Iteration 275, loss = 0.09021495\n",
      "Iteration 276, loss = 0.08988478\n",
      "Iteration 277, loss = 0.08980361\n",
      "Iteration 278, loss = 0.08922867\n",
      "Iteration 279, loss = 0.08903216\n",
      "Iteration 280, loss = 0.08859472\n",
      "Iteration 281, loss = 0.08848865\n",
      "Iteration 282, loss = 0.08813763\n",
      "Iteration 283, loss = 0.08774182\n",
      "Iteration 284, loss = 0.08718242\n",
      "Iteration 285, loss = 0.08689473\n",
      "Iteration 286, loss = 0.08654621\n",
      "Iteration 287, loss = 0.08662759\n",
      "Iteration 288, loss = 0.08587895\n",
      "Iteration 289, loss = 0.08564765\n",
      "Iteration 290, loss = 0.08528373\n",
      "Iteration 291, loss = 0.08539983\n",
      "Iteration 292, loss = 0.08455922\n",
      "Iteration 293, loss = 0.08435223\n",
      "Iteration 294, loss = 0.08431475\n",
      "Iteration 295, loss = 0.08383137\n",
      "Iteration 296, loss = 0.08344068\n",
      "Iteration 297, loss = 0.08306672\n",
      "Iteration 298, loss = 0.08282784\n",
      "Iteration 299, loss = 0.08248269\n",
      "Iteration 300, loss = 0.08212771\n",
      "Iteration 301, loss = 0.08195869\n",
      "Iteration 302, loss = 0.08174338\n",
      "Iteration 303, loss = 0.08140884\n",
      "Iteration 304, loss = 0.08092332\n",
      "Iteration 305, loss = 0.08078913\n",
      "Iteration 306, loss = 0.08049536\n",
      "Iteration 307, loss = 0.08018828\n",
      "Iteration 308, loss = 0.07982194\n",
      "Iteration 309, loss = 0.07964499\n",
      "Iteration 310, loss = 0.07933967\n",
      "Iteration 311, loss = 0.07898073\n",
      "Iteration 312, loss = 0.07871467\n",
      "Iteration 313, loss = 0.07872255\n",
      "Iteration 314, loss = 0.07819280\n",
      "Iteration 315, loss = 0.07793174\n",
      "Iteration 316, loss = 0.07751785\n",
      "Iteration 317, loss = 0.07730533\n",
      "Iteration 318, loss = 0.07709286\n",
      "Iteration 319, loss = 0.07690516\n",
      "Iteration 320, loss = 0.07652792\n",
      "Iteration 321, loss = 0.07640680\n",
      "Iteration 322, loss = 0.07599138\n",
      "Iteration 323, loss = 0.07578896\n",
      "Iteration 324, loss = 0.07548018\n",
      "Iteration 325, loss = 0.07541269\n",
      "Iteration 326, loss = 0.07511005\n",
      "Iteration 327, loss = 0.07473229\n",
      "Iteration 328, loss = 0.07456032\n",
      "Iteration 329, loss = 0.07430492\n",
      "Iteration 330, loss = 0.07401403\n",
      "Iteration 331, loss = 0.07388205\n",
      "Iteration 332, loss = 0.07342539\n",
      "Iteration 333, loss = 0.07324338\n",
      "Iteration 334, loss = 0.07298010\n",
      "Iteration 335, loss = 0.07277984\n",
      "Iteration 336, loss = 0.07244602\n",
      "Iteration 337, loss = 0.07227038\n",
      "Iteration 338, loss = 0.07200274\n",
      "Iteration 339, loss = 0.07184834\n",
      "Iteration 340, loss = 0.07147584\n",
      "Iteration 341, loss = 0.07124547\n",
      "Iteration 342, loss = 0.07104023\n",
      "Iteration 343, loss = 0.07080402\n",
      "Iteration 344, loss = 0.07125581\n",
      "Iteration 345, loss = 0.07073280\n",
      "Iteration 346, loss = 0.07048684\n",
      "Iteration 347, loss = 0.06986139\n",
      "Iteration 348, loss = 0.06962434\n",
      "Iteration 349, loss = 0.06937024\n",
      "Iteration 350, loss = 0.06925658\n",
      "Iteration 351, loss = 0.06894089\n",
      "Iteration 352, loss = 0.06874634\n",
      "Iteration 353, loss = 0.06848613\n",
      "Iteration 354, loss = 0.06839130\n",
      "Iteration 355, loss = 0.06810763\n",
      "Iteration 356, loss = 0.06799831\n",
      "Iteration 357, loss = 0.06762902\n",
      "Iteration 358, loss = 0.06745935\n",
      "Iteration 359, loss = 0.06726407\n",
      "Iteration 360, loss = 0.06703320\n",
      "Iteration 361, loss = 0.06683251\n",
      "Iteration 362, loss = 0.06659403\n",
      "Iteration 363, loss = 0.06654898\n",
      "Iteration 364, loss = 0.06614907\n",
      "Iteration 365, loss = 0.06598120\n",
      "Iteration 366, loss = 0.06578078\n",
      "Iteration 367, loss = 0.06563822\n",
      "Iteration 368, loss = 0.06541494\n",
      "Iteration 369, loss = 0.06539047\n",
      "Iteration 370, loss = 0.06497761\n",
      "Iteration 371, loss = 0.06490621\n",
      "Iteration 372, loss = 0.06469495\n",
      "Iteration 373, loss = 0.06434952\n",
      "Iteration 374, loss = 0.06422161\n",
      "Iteration 375, loss = 0.06396052\n",
      "Iteration 376, loss = 0.06374009\n",
      "Iteration 377, loss = 0.06363720\n",
      "Iteration 378, loss = 0.06344344\n",
      "Iteration 379, loss = 0.06316742\n",
      "Iteration 380, loss = 0.06299805\n",
      "Iteration 381, loss = 0.06275740\n",
      "Iteration 382, loss = 0.06257150\n",
      "Iteration 383, loss = 0.06236143\n",
      "Iteration 384, loss = 0.06222748\n",
      "Iteration 385, loss = 0.06199867\n",
      "Iteration 386, loss = 0.06192815\n",
      "Iteration 387, loss = 0.06164860\n",
      "Iteration 388, loss = 0.06141679\n",
      "Iteration 389, loss = 0.06118127\n",
      "Iteration 390, loss = 0.06118241\n",
      "Iteration 391, loss = 0.06104935\n",
      "Iteration 392, loss = 0.06089234\n",
      "Iteration 393, loss = 0.06059149\n",
      "Iteration 394, loss = 0.06045236\n",
      "Iteration 395, loss = 0.06033648\n",
      "Iteration 396, loss = 0.05991938\n",
      "Iteration 397, loss = 0.05979440\n",
      "Iteration 398, loss = 0.05956726\n",
      "Iteration 399, loss = 0.05940072\n",
      "Iteration 400, loss = 0.05925484\n",
      "Iteration 401, loss = 0.05919624\n",
      "Iteration 402, loss = 0.05899200\n",
      "Iteration 403, loss = 0.05866866\n",
      "Iteration 404, loss = 0.05848265\n",
      "Iteration 405, loss = 0.05837713\n",
      "Iteration 406, loss = 0.05816404\n",
      "Iteration 407, loss = 0.05801809\n",
      "Iteration 408, loss = 0.05795980\n",
      "Iteration 409, loss = 0.05768929\n",
      "Iteration 410, loss = 0.05759420\n",
      "Iteration 411, loss = 0.05736904\n",
      "Iteration 412, loss = 0.05722594\n",
      "Iteration 413, loss = 0.05700605\n",
      "Iteration 414, loss = 0.05684209\n",
      "Iteration 415, loss = 0.05669274\n",
      "Iteration 416, loss = 0.05654606\n",
      "Iteration 417, loss = 0.05649402\n",
      "Iteration 418, loss = 0.05622501\n",
      "Iteration 419, loss = 0.05616631\n",
      "Iteration 420, loss = 0.05599419\n",
      "Iteration 421, loss = 0.05580240\n",
      "Iteration 422, loss = 0.05560332\n",
      "Iteration 423, loss = 0.05549752\n",
      "Iteration 424, loss = 0.05526567\n",
      "Iteration 425, loss = 0.05515403\n",
      "Iteration 426, loss = 0.05496878\n",
      "Iteration 427, loss = 0.05481977\n",
      "Iteration 428, loss = 0.05474449\n",
      "Iteration 429, loss = 0.05451579\n",
      "Iteration 430, loss = 0.05451039\n",
      "Iteration 431, loss = 0.05425435\n",
      "Iteration 432, loss = 0.05417801\n",
      "Iteration 433, loss = 0.05408456\n",
      "Iteration 434, loss = 0.05389586\n",
      "Iteration 435, loss = 0.05370826\n",
      "Iteration 436, loss = 0.05352963\n",
      "Iteration 437, loss = 0.05341446\n",
      "Iteration 438, loss = 0.05318327\n",
      "Iteration 439, loss = 0.05309283\n",
      "Iteration 440, loss = 0.05295993\n",
      "Iteration 441, loss = 0.05282584\n",
      "Iteration 442, loss = 0.05268442\n",
      "Iteration 443, loss = 0.05251154\n",
      "Iteration 444, loss = 0.05237169\n",
      "Iteration 445, loss = 0.05217399\n",
      "Iteration 446, loss = 0.05220308\n",
      "Iteration 447, loss = 0.05202286\n",
      "Iteration 448, loss = 0.05195199\n",
      "Iteration 449, loss = 0.05184263\n",
      "Iteration 450, loss = 0.05150955\n",
      "Iteration 451, loss = 0.05152133\n",
      "Iteration 452, loss = 0.05149212\n",
      "Iteration 453, loss = 0.05118529\n",
      "Iteration 454, loss = 0.05107612\n",
      "Iteration 455, loss = 0.05088117\n",
      "Iteration 456, loss = 0.05073671\n",
      "Iteration 457, loss = 0.05070667\n",
      "Iteration 458, loss = 0.05055949\n",
      "Iteration 459, loss = 0.05043626\n",
      "Iteration 460, loss = 0.05020086\n",
      "Iteration 461, loss = 0.05016586\n",
      "Iteration 462, loss = 0.04990807\n",
      "Iteration 463, loss = 0.04995001\n",
      "Iteration 464, loss = 0.04971774\n",
      "Iteration 465, loss = 0.04956658\n",
      "Iteration 466, loss = 0.04947365\n",
      "Iteration 467, loss = 0.04930578\n",
      "Iteration 468, loss = 0.04920202\n",
      "Iteration 469, loss = 0.04912131\n",
      "Iteration 470, loss = 0.04891953\n",
      "Iteration 471, loss = 0.04884029\n",
      "Iteration 472, loss = 0.04874957\n",
      "Iteration 473, loss = 0.04850034\n",
      "Iteration 474, loss = 0.04843704\n",
      "Iteration 475, loss = 0.04832768\n",
      "Iteration 476, loss = 0.04842398\n",
      "Iteration 477, loss = 0.04807454\n",
      "Iteration 478, loss = 0.04802134\n",
      "Iteration 479, loss = 0.04790389\n",
      "Iteration 480, loss = 0.04778990\n",
      "Iteration 481, loss = 0.04764487\n",
      "Iteration 482, loss = 0.04746006\n",
      "Iteration 483, loss = 0.04733967\n",
      "Iteration 484, loss = 0.04729836\n",
      "Iteration 485, loss = 0.04731216\n",
      "Iteration 486, loss = 0.04704843\n",
      "Iteration 487, loss = 0.04691788\n",
      "Iteration 488, loss = 0.04675082\n",
      "Iteration 489, loss = 0.04662660\n",
      "Iteration 490, loss = 0.04658091\n",
      "Iteration 491, loss = 0.04651984\n",
      "Iteration 492, loss = 0.04646926\n",
      "Iteration 493, loss = 0.04622930\n",
      "Iteration 494, loss = 0.04617237\n",
      "Iteration 495, loss = 0.04596625\n",
      "Iteration 496, loss = 0.04599703\n",
      "Iteration 497, loss = 0.04580462\n",
      "Iteration 498, loss = 0.04563905\n",
      "Iteration 499, loss = 0.04555951\n",
      "Iteration 500, loss = 0.04552636\n",
      "Iteration 501, loss = 0.04533916\n",
      "Iteration 502, loss = 0.04524715\n",
      "Iteration 503, loss = 0.04513414\n",
      "Iteration 504, loss = 0.04502811\n",
      "Iteration 505, loss = 0.04494095\n",
      "Iteration 506, loss = 0.04472760\n",
      "Iteration 507, loss = 0.04464028\n",
      "Iteration 508, loss = 0.04448764\n",
      "Iteration 509, loss = 0.04441315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 510, loss = 0.04436056\n",
      "Iteration 511, loss = 0.04421059\n",
      "Iteration 512, loss = 0.04424375\n",
      "Iteration 513, loss = 0.04430291\n",
      "Iteration 514, loss = 0.04391584\n",
      "Iteration 515, loss = 0.04386262\n",
      "Iteration 516, loss = 0.04375708\n",
      "Iteration 517, loss = 0.04374958\n",
      "Iteration 518, loss = 0.04349202\n",
      "Iteration 519, loss = 0.04334859\n",
      "Iteration 520, loss = 0.04333269\n",
      "Iteration 521, loss = 0.04319013\n",
      "Iteration 522, loss = 0.04311063\n",
      "Iteration 523, loss = 0.04303144\n",
      "Iteration 524, loss = 0.04293049\n",
      "Iteration 525, loss = 0.04282548\n",
      "Iteration 526, loss = 0.04284770\n",
      "Iteration 527, loss = 0.04256051\n",
      "Iteration 528, loss = 0.04255968\n",
      "Iteration 529, loss = 0.04242470\n",
      "Iteration 530, loss = 0.04225813\n",
      "Iteration 531, loss = 0.04219547\n",
      "Iteration 532, loss = 0.04209690\n",
      "Iteration 533, loss = 0.04196078\n",
      "Iteration 534, loss = 0.04193697\n",
      "Iteration 535, loss = 0.04180925\n",
      "Iteration 536, loss = 0.04169524\n",
      "Iteration 537, loss = 0.04163551\n",
      "Iteration 538, loss = 0.04152136\n",
      "Iteration 539, loss = 0.04138693\n",
      "Iteration 540, loss = 0.04141054\n",
      "Iteration 541, loss = 0.04134496\n",
      "Iteration 542, loss = 0.04115404\n",
      "Iteration 543, loss = 0.04111309\n",
      "Iteration 544, loss = 0.04099091\n",
      "Iteration 545, loss = 0.04092823\n",
      "Iteration 546, loss = 0.04074533\n",
      "Iteration 547, loss = 0.04067416\n",
      "Iteration 548, loss = 0.04069028\n",
      "Iteration 549, loss = 0.04072671\n",
      "Iteration 550, loss = 0.04057461\n",
      "Iteration 551, loss = 0.04031970\n",
      "Iteration 552, loss = 0.04019808\n",
      "Iteration 553, loss = 0.04012698\n",
      "Iteration 554, loss = 0.04006013\n",
      "Iteration 555, loss = 0.03998355\n",
      "Iteration 556, loss = 0.03987375\n",
      "Iteration 557, loss = 0.03994404\n",
      "Iteration 558, loss = 0.03973132\n",
      "Iteration 559, loss = 0.03971807\n",
      "Iteration 560, loss = 0.03951823\n",
      "Iteration 561, loss = 0.03940702\n",
      "Iteration 562, loss = 0.03934295\n",
      "Iteration 563, loss = 0.03924951\n",
      "Iteration 564, loss = 0.03929489\n",
      "Iteration 565, loss = 0.03901949\n",
      "Iteration 566, loss = 0.03914633\n",
      "Iteration 567, loss = 0.03902442\n",
      "Iteration 568, loss = 0.03878583\n",
      "Iteration 569, loss = 0.03874373\n",
      "Iteration 570, loss = 0.03873969\n",
      "Iteration 571, loss = 0.03857254\n",
      "Iteration 572, loss = 0.03849869\n",
      "Iteration 573, loss = 0.03840077\n",
      "Iteration 574, loss = 0.03836342\n",
      "Iteration 575, loss = 0.03830851\n",
      "Iteration 576, loss = 0.03816489\n",
      "Iteration 577, loss = 0.03807696\n",
      "Iteration 578, loss = 0.03805649\n",
      "Iteration 579, loss = 0.03795486\n",
      "Iteration 580, loss = 0.03785057\n",
      "Iteration 581, loss = 0.03773720\n",
      "Iteration 582, loss = 0.03771184\n",
      "Iteration 583, loss = 0.03762627\n",
      "Iteration 584, loss = 0.03753607\n",
      "Iteration 585, loss = 0.03759999\n",
      "Iteration 586, loss = 0.03733366\n",
      "Iteration 587, loss = 0.03727071\n",
      "Iteration 588, loss = 0.03735425\n",
      "Iteration 589, loss = 0.03722719\n",
      "Iteration 590, loss = 0.03713973\n",
      "Iteration 591, loss = 0.03692595\n",
      "Iteration 592, loss = 0.03691651\n",
      "Iteration 593, loss = 0.03683976\n",
      "Iteration 594, loss = 0.03675217\n",
      "Iteration 595, loss = 0.03667997\n",
      "Iteration 596, loss = 0.03669798\n",
      "Iteration 597, loss = 0.03650440\n",
      "Iteration 598, loss = 0.03653289\n",
      "Iteration 599, loss = 0.03636755\n",
      "Iteration 600, loss = 0.03634006\n",
      "Iteration 601, loss = 0.03617617\n",
      "Iteration 602, loss = 0.03626254\n",
      "Iteration 603, loss = 0.03601933\n",
      "Iteration 604, loss = 0.03606452\n",
      "Iteration 605, loss = 0.03601906\n",
      "Iteration 606, loss = 0.03584004\n",
      "Iteration 607, loss = 0.03612441\n",
      "Iteration 608, loss = 0.03570974\n",
      "Iteration 609, loss = 0.03579305\n",
      "Iteration 610, loss = 0.03557845\n",
      "Iteration 611, loss = 0.03543490\n",
      "Iteration 612, loss = 0.03542084\n",
      "Iteration 613, loss = 0.03529204\n",
      "Iteration 614, loss = 0.03522548\n",
      "Iteration 615, loss = 0.03512208\n",
      "Iteration 616, loss = 0.03513088\n",
      "Iteration 617, loss = 0.03506082\n",
      "Iteration 618, loss = 0.03501456\n",
      "Iteration 619, loss = 0.03491245\n",
      "Iteration 620, loss = 0.03480776\n",
      "Iteration 621, loss = 0.03471810\n",
      "Iteration 622, loss = 0.03464313\n",
      "Iteration 623, loss = 0.03472317\n",
      "Iteration 624, loss = 0.03452679\n",
      "Iteration 625, loss = 0.03451476\n",
      "Iteration 626, loss = 0.03444109\n",
      "Iteration 627, loss = 0.03434917\n",
      "Iteration 628, loss = 0.03432327\n",
      "Iteration 629, loss = 0.03434917\n",
      "Iteration 630, loss = 0.03416727\n",
      "Iteration 631, loss = 0.03403683\n",
      "Iteration 632, loss = 0.03398079\n",
      "Iteration 633, loss = 0.03390913\n",
      "Iteration 634, loss = 0.03392029\n",
      "Iteration 635, loss = 0.03375581\n",
      "Iteration 636, loss = 0.03370019\n",
      "Iteration 637, loss = 0.03368088\n",
      "Iteration 638, loss = 0.03356108\n",
      "Iteration 639, loss = 0.03355929\n",
      "Iteration 640, loss = 0.03343150\n",
      "Iteration 641, loss = 0.03335606\n",
      "Iteration 642, loss = 0.03340343\n",
      "Iteration 643, loss = 0.03327622\n",
      "Iteration 644, loss = 0.03328416\n",
      "Iteration 645, loss = 0.03314549\n",
      "Iteration 646, loss = 0.03305982\n",
      "Iteration 647, loss = 0.03295325\n",
      "Iteration 648, loss = 0.03289788\n",
      "Iteration 649, loss = 0.03287353\n",
      "Iteration 650, loss = 0.03279796\n",
      "Iteration 651, loss = 0.03279998\n",
      "Iteration 652, loss = 0.03272135\n",
      "Iteration 653, loss = 0.03259959\n",
      "Iteration 654, loss = 0.03259681\n",
      "Iteration 655, loss = 0.03248171\n",
      "Iteration 656, loss = 0.03243012\n",
      "Iteration 657, loss = 0.03235982\n",
      "Iteration 658, loss = 0.03227063\n",
      "Iteration 659, loss = 0.03225683\n",
      "Iteration 660, loss = 0.03223456\n",
      "Iteration 661, loss = 0.03220836\n",
      "Iteration 662, loss = 0.03207302\n",
      "Iteration 663, loss = 0.03195666\n",
      "Iteration 664, loss = 0.03190751\n",
      "Iteration 665, loss = 0.03187317\n",
      "Iteration 666, loss = 0.03179096\n",
      "Iteration 667, loss = 0.03175915\n",
      "Iteration 668, loss = 0.03168649\n",
      "Iteration 669, loss = 0.03165835\n",
      "Iteration 670, loss = 0.03166393\n",
      "Iteration 671, loss = 0.03161266\n",
      "Iteration 672, loss = 0.03148503\n",
      "Iteration 673, loss = 0.03139906\n",
      "Iteration 674, loss = 0.03134319\n",
      "Iteration 675, loss = 0.03126966\n",
      "Iteration 676, loss = 0.03119104\n",
      "Iteration 677, loss = 0.03114456\n",
      "Iteration 678, loss = 0.03128000\n",
      "Iteration 679, loss = 0.03101258\n",
      "Iteration 680, loss = 0.03106373\n",
      "Iteration 681, loss = 0.03108056\n",
      "Iteration 682, loss = 0.03101663\n",
      "Iteration 683, loss = 0.03087628\n",
      "Iteration 684, loss = 0.03071970\n",
      "Iteration 685, loss = 0.03069743\n",
      "Iteration 686, loss = 0.03068820\n",
      "Iteration 687, loss = 0.03065581\n",
      "Iteration 688, loss = 0.03053789\n",
      "Iteration 689, loss = 0.03046029\n",
      "Iteration 690, loss = 0.03047533\n",
      "Iteration 691, loss = 0.03036718\n",
      "Iteration 692, loss = 0.03028618\n",
      "Iteration 693, loss = 0.03026703\n",
      "Iteration 694, loss = 0.03024782\n",
      "Iteration 695, loss = 0.03012470\n",
      "Iteration 696, loss = 0.03003400\n",
      "Iteration 697, loss = 0.03006081\n",
      "Iteration 698, loss = 0.03002398\n",
      "Iteration 699, loss = 0.02995192\n",
      "Iteration 700, loss = 0.02986316\n",
      "Iteration 701, loss = 0.02992116\n",
      "Iteration 702, loss = 0.02973840\n",
      "Iteration 703, loss = 0.02977874\n",
      "Iteration 704, loss = 0.02964779\n",
      "Iteration 705, loss = 0.02960421\n",
      "Iteration 706, loss = 0.02950339\n",
      "Iteration 707, loss = 0.02946032\n",
      "Iteration 708, loss = 0.02945827\n",
      "Iteration 709, loss = 0.02937024\n",
      "Iteration 710, loss = 0.02935831\n",
      "Iteration 711, loss = 0.02924624\n",
      "Iteration 712, loss = 0.02929374\n",
      "Iteration 713, loss = 0.02912193\n",
      "Iteration 714, loss = 0.02912190\n",
      "Iteration 715, loss = 0.02904509\n",
      "Iteration 716, loss = 0.02895611\n",
      "Iteration 717, loss = 0.02892827\n",
      "Iteration 718, loss = 0.02891482\n",
      "Iteration 719, loss = 0.02881875\n",
      "Iteration 720, loss = 0.02903984\n",
      "Iteration 721, loss = 0.02876078\n",
      "Iteration 722, loss = 0.02869593\n",
      "Iteration 723, loss = 0.02861973\n",
      "Iteration 724, loss = 0.02857373\n",
      "Iteration 725, loss = 0.02850517\n",
      "Iteration 726, loss = 0.02850804\n",
      "Iteration 727, loss = 0.02842509\n",
      "Iteration 728, loss = 0.02839730\n",
      "Iteration 729, loss = 0.02829670\n",
      "Iteration 730, loss = 0.02828783\n",
      "Iteration 731, loss = 0.02823906\n",
      "Iteration 732, loss = 0.02816143\n",
      "Iteration 733, loss = 0.02809843\n",
      "Iteration 734, loss = 0.02810767\n",
      "Iteration 735, loss = 0.02804306\n",
      "Iteration 736, loss = 0.02795799\n",
      "Iteration 737, loss = 0.02794742\n",
      "Iteration 738, loss = 0.02790912\n",
      "Iteration 739, loss = 0.02780833\n",
      "Iteration 740, loss = 0.02776127\n",
      "Iteration 741, loss = 0.02772004\n",
      "Iteration 742, loss = 0.02769853\n",
      "Iteration 743, loss = 0.02762343\n",
      "Iteration 744, loss = 0.02758488\n",
      "Iteration 745, loss = 0.02751260\n",
      "Iteration 746, loss = 0.02748080\n",
      "Iteration 747, loss = 0.02745227\n",
      "Iteration 748, loss = 0.02740307\n",
      "Iteration 749, loss = 0.02743436\n",
      "Iteration 750, loss = 0.02730680\n",
      "Iteration 751, loss = 0.02726457\n",
      "Iteration 752, loss = 0.02722434\n",
      "Iteration 753, loss = 0.02722674\n",
      "Iteration 754, loss = 0.02715587\n",
      "Iteration 755, loss = 0.02716415\n",
      "Iteration 756, loss = 0.02704160\n",
      "Iteration 757, loss = 0.02700064\n",
      "Iteration 758, loss = 0.02694809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 759, loss = 0.02691545\n",
      "Iteration 760, loss = 0.02682512\n",
      "Iteration 761, loss = 0.02685780\n",
      "Iteration 762, loss = 0.02672969\n",
      "Iteration 763, loss = 0.02667765\n",
      "Iteration 764, loss = 0.02678234\n",
      "Iteration 765, loss = 0.02658658\n",
      "Iteration 766, loss = 0.02669530\n",
      "Iteration 767, loss = 0.02651134\n",
      "Iteration 768, loss = 0.02651022\n",
      "Iteration 769, loss = 0.02648192\n",
      "Iteration 770, loss = 0.02649021\n",
      "Iteration 771, loss = 0.02632580\n",
      "Iteration 772, loss = 0.02649864\n",
      "Iteration 773, loss = 0.02630332\n",
      "Iteration 774, loss = 0.02637662\n",
      "Iteration 775, loss = 0.02616613\n",
      "Iteration 776, loss = 0.02614651\n",
      "Iteration 777, loss = 0.02612389\n",
      "Iteration 778, loss = 0.02608682\n",
      "Iteration 779, loss = 0.02615297\n",
      "Iteration 780, loss = 0.02605590\n",
      "Iteration 781, loss = 0.02588750\n",
      "Iteration 782, loss = 0.02588154\n",
      "Iteration 783, loss = 0.02592150\n",
      "Iteration 784, loss = 0.02577807\n",
      "Iteration 785, loss = 0.02580678\n",
      "Iteration 786, loss = 0.02569069\n",
      "Iteration 787, loss = 0.02573322\n",
      "Iteration 788, loss = 0.02563384\n",
      "Iteration 789, loss = 0.02556254\n",
      "Iteration 790, loss = 0.02555728\n",
      "Iteration 791, loss = 0.02557096\n",
      "Iteration 792, loss = 0.02568456\n",
      "Iteration 793, loss = 0.02538940\n",
      "Iteration 794, loss = 0.02544095\n",
      "Iteration 795, loss = 0.02534332\n",
      "Iteration 796, loss = 0.02534843\n",
      "Iteration 797, loss = 0.02522265\n",
      "Iteration 798, loss = 0.02520213\n",
      "Iteration 799, loss = 0.02522034\n",
      "Iteration 800, loss = 0.02510699\n",
      "Iteration 801, loss = 0.02507258\n",
      "Iteration 802, loss = 0.02503177\n",
      "Iteration 803, loss = 0.02502481\n",
      "Iteration 804, loss = 0.02496524\n",
      "Iteration 805, loss = 0.02492287\n",
      "Iteration 806, loss = 0.02487252\n",
      "Iteration 807, loss = 0.02484285\n",
      "Iteration 808, loss = 0.02481861\n",
      "Iteration 809, loss = 0.02475539\n",
      "Iteration 810, loss = 0.02472811\n",
      "Iteration 811, loss = 0.02471383\n",
      "Iteration 812, loss = 0.02472999\n",
      "Iteration 813, loss = 0.02459476\n",
      "Iteration 814, loss = 0.02460837\n",
      "Iteration 815, loss = 0.02453052\n",
      "Iteration 816, loss = 0.02452230\n",
      "Iteration 817, loss = 0.02449907\n",
      "Iteration 818, loss = 0.02449460\n",
      "Iteration 819, loss = 0.02445870\n",
      "Iteration 820, loss = 0.02437314\n",
      "Iteration 821, loss = 0.02431153\n",
      "Iteration 822, loss = 0.02425798\n",
      "Iteration 823, loss = 0.02423239\n",
      "Iteration 824, loss = 0.02416915\n",
      "Iteration 825, loss = 0.02414759\n",
      "Iteration 826, loss = 0.02411552\n",
      "Iteration 827, loss = 0.02407209\n",
      "Iteration 828, loss = 0.02416282\n",
      "Iteration 829, loss = 0.02404527\n",
      "Iteration 830, loss = 0.02397141\n",
      "Iteration 831, loss = 0.02391556\n",
      "Iteration 832, loss = 0.02388524\n",
      "Iteration 833, loss = 0.02394633\n",
      "Iteration 834, loss = 0.02383036\n",
      "Iteration 835, loss = 0.02375650\n",
      "Iteration 836, loss = 0.02372550\n",
      "Iteration 837, loss = 0.02370294\n",
      "Iteration 838, loss = 0.02364984\n",
      "Iteration 839, loss = 0.02365257\n",
      "Iteration 840, loss = 0.02359259\n",
      "Iteration 841, loss = 0.02357642\n",
      "Iteration 842, loss = 0.02350104\n",
      "Iteration 843, loss = 0.02356928\n",
      "Iteration 844, loss = 0.02350236\n",
      "Iteration 845, loss = 0.02348533\n",
      "Iteration 846, loss = 0.02343757\n",
      "Iteration 847, loss = 0.02333209\n",
      "Iteration 848, loss = 0.02329451\n",
      "Iteration 849, loss = 0.02327392\n",
      "Iteration 850, loss = 0.02324712\n",
      "Iteration 851, loss = 0.02318455\n",
      "Iteration 852, loss = 0.02317159\n",
      "Iteration 853, loss = 0.02313385\n",
      "Iteration 854, loss = 0.02313024\n",
      "Iteration 855, loss = 0.02305438\n",
      "Iteration 856, loss = 0.02306711\n",
      "Iteration 857, loss = 0.02298074\n",
      "Iteration 858, loss = 0.02296199\n",
      "Iteration 859, loss = 0.02289705\n",
      "Iteration 860, loss = 0.02288361\n",
      "Iteration 861, loss = 0.02283231\n",
      "Iteration 862, loss = 0.02283668\n",
      "Iteration 863, loss = 0.02287765\n",
      "Iteration 864, loss = 0.02272820\n",
      "Iteration 865, loss = 0.02270875\n",
      "Iteration 866, loss = 0.02271220\n",
      "Iteration 867, loss = 0.02265975\n",
      "Iteration 868, loss = 0.02260065\n",
      "Iteration 869, loss = 0.02258209\n",
      "Iteration 870, loss = 0.02257325\n",
      "Iteration 871, loss = 0.02266146\n",
      "Iteration 872, loss = 0.02250048\n",
      "Iteration 873, loss = 0.02250262\n",
      "Iteration 874, loss = 0.02239932\n",
      "Iteration 875, loss = 0.02235909\n",
      "Iteration 876, loss = 0.02231645\n",
      "Iteration 877, loss = 0.02229758\n",
      "Iteration 878, loss = 0.02226123\n",
      "Iteration 879, loss = 0.02226729\n",
      "Iteration 880, loss = 0.02220617\n",
      "Iteration 881, loss = 0.02218265\n",
      "Iteration 882, loss = 0.02219290\n",
      "Iteration 883, loss = 0.02210410\n",
      "Iteration 884, loss = 0.02212933\n",
      "Iteration 885, loss = 0.02204836\n",
      "Iteration 886, loss = 0.02199873\n",
      "Iteration 887, loss = 0.02199340\n",
      "Iteration 888, loss = 0.02201226\n",
      "Iteration 889, loss = 0.02194517\n",
      "Iteration 890, loss = 0.02188901\n",
      "Iteration 891, loss = 0.02193441\n",
      "Iteration 892, loss = 0.02186622\n",
      "Iteration 893, loss = 0.02183474\n",
      "Iteration 894, loss = 0.02179765\n",
      "Iteration 895, loss = 0.02172126\n",
      "Iteration 896, loss = 0.02171865\n",
      "Iteration 897, loss = 0.02169929\n",
      "Iteration 898, loss = 0.02162502\n",
      "Iteration 899, loss = 0.02161201\n",
      "Iteration 900, loss = 0.02156167\n",
      "Iteration 901, loss = 0.02160386\n",
      "Iteration 902, loss = 0.02160120\n",
      "Iteration 903, loss = 0.02152010\n",
      "Iteration 904, loss = 0.02147303\n",
      "Iteration 905, loss = 0.02141391\n",
      "Iteration 906, loss = 0.02140985\n",
      "Iteration 907, loss = 0.02136439\n",
      "Iteration 908, loss = 0.02132634\n",
      "Iteration 909, loss = 0.02128238\n",
      "Iteration 910, loss = 0.02127552\n",
      "Iteration 911, loss = 0.02131530\n",
      "Iteration 912, loss = 0.02123347\n",
      "Iteration 913, loss = 0.02114892\n",
      "Iteration 914, loss = 0.02112500\n",
      "Iteration 915, loss = 0.02111957\n",
      "Iteration 916, loss = 0.02107268\n",
      "Iteration 917, loss = 0.02104541\n",
      "Iteration 918, loss = 0.02106598\n",
      "Iteration 919, loss = 0.02097376\n",
      "Iteration 920, loss = 0.02094377\n",
      "Iteration 921, loss = 0.02098292\n",
      "Iteration 922, loss = 0.02090100\n",
      "Iteration 923, loss = 0.02093737\n",
      "Iteration 924, loss = 0.02087879\n",
      "Iteration 925, loss = 0.02079604\n",
      "Iteration 926, loss = 0.02082370\n",
      "Iteration 927, loss = 0.02074964\n",
      "Iteration 928, loss = 0.02075482\n",
      "Iteration 929, loss = 0.02086745\n",
      "Iteration 930, loss = 0.02070833\n",
      "Iteration 931, loss = 0.02067834\n",
      "Iteration 932, loss = 0.02061992\n",
      "Iteration 933, loss = 0.02063307\n",
      "Iteration 934, loss = 0.02061490\n",
      "Iteration 935, loss = 0.02052304\n",
      "Iteration 936, loss = 0.02047849\n",
      "Iteration 937, loss = 0.02049387\n",
      "Iteration 938, loss = 0.02047384\n",
      "Iteration 939, loss = 0.02049461\n",
      "Iteration 940, loss = 0.02038756\n",
      "Iteration 941, loss = 0.02038028\n",
      "Iteration 942, loss = 0.02036585\n",
      "Iteration 943, loss = 0.02032480\n",
      "Iteration 944, loss = 0.02026535\n",
      "Iteration 945, loss = 0.02025308\n",
      "Iteration 946, loss = 0.02020665\n",
      "Iteration 947, loss = 0.02022361\n",
      "Iteration 948, loss = 0.02018044\n",
      "Iteration 949, loss = 0.02012810\n",
      "Iteration 950, loss = 0.02010337\n",
      "Iteration 951, loss = 0.02007677\n",
      "Iteration 952, loss = 0.02004897\n",
      "Iteration 953, loss = 0.02000418\n",
      "Iteration 954, loss = 0.02001511\n",
      "Iteration 955, loss = 0.01998355\n",
      "Iteration 956, loss = 0.01995546\n",
      "Iteration 957, loss = 0.02001817\n",
      "Iteration 958, loss = 0.01991032\n",
      "Iteration 959, loss = 0.01985083\n",
      "Iteration 960, loss = 0.01996559\n",
      "Iteration 961, loss = 0.01983854\n",
      "Iteration 962, loss = 0.01976544\n",
      "Iteration 963, loss = 0.01973996\n",
      "Iteration 964, loss = 0.01974132\n",
      "Iteration 965, loss = 0.01971292\n",
      "Iteration 966, loss = 0.01972832\n",
      "Iteration 967, loss = 0.01963789\n",
      "Iteration 968, loss = 0.01960465\n",
      "Iteration 969, loss = 0.01973381\n",
      "Iteration 970, loss = 0.01955370\n",
      "Iteration 971, loss = 0.01955942\n",
      "Iteration 972, loss = 0.01952666\n",
      "Iteration 973, loss = 0.01948642\n",
      "Iteration 974, loss = 0.01948876\n",
      "Iteration 975, loss = 0.01944103\n",
      "Iteration 976, loss = 0.01942197\n",
      "Iteration 977, loss = 0.01936965\n",
      "Iteration 978, loss = 0.01938617\n",
      "Iteration 979, loss = 0.01934851\n",
      "Iteration 980, loss = 0.01933658\n",
      "Iteration 981, loss = 0.01930353\n",
      "Iteration 982, loss = 0.01925240\n",
      "Iteration 983, loss = 0.01923761\n",
      "Iteration 984, loss = 0.01925214\n",
      "Iteration 985, loss = 0.01918372\n",
      "Iteration 986, loss = 0.01914690\n",
      "Iteration 987, loss = 0.01914780\n",
      "Iteration 988, loss = 0.01910684\n",
      "Iteration 989, loss = 0.01907930\n",
      "Iteration 990, loss = 0.01906308\n",
      "Iteration 991, loss = 0.01904989\n",
      "Iteration 992, loss = 0.01899846\n",
      "Iteration 993, loss = 0.01899998\n",
      "Iteration 994, loss = 0.01903906\n",
      "Iteration 995, loss = 0.01891730\n",
      "Iteration 996, loss = 0.01893380\n",
      "Iteration 997, loss = 0.01892280\n",
      "Iteration 998, loss = 0.01886978\n",
      "Iteration 999, loss = 0.01881901\n",
      "Iteration 1000, loss = 0.01883000\n"
     ]
    }
   ],
   "source": [
    "nnModelSGD= mlpSGD.fit(trainData, trainLabelE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(90,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='sgd', tol=1e-19,\n",
       "              validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnModelSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.70992586\n",
      "Iteration 2, loss = 1.13363724\n",
      "Iteration 3, loss = 0.88346770\n",
      "Iteration 4, loss = 0.71665047\n",
      "Iteration 5, loss = 0.59934756\n",
      "Iteration 6, loss = 0.50793627\n",
      "Iteration 7, loss = 0.44334715\n",
      "Iteration 8, loss = 0.38033511\n",
      "Iteration 9, loss = 0.34191679\n",
      "Iteration 10, loss = 0.30432116\n",
      "Iteration 11, loss = 0.27342682\n",
      "Iteration 12, loss = 0.25322429\n",
      "Iteration 13, loss = 0.22978753\n",
      "Iteration 14, loss = 0.20540610\n",
      "Iteration 15, loss = 0.18662989\n",
      "Iteration 16, loss = 0.17422244\n",
      "Iteration 17, loss = 0.16331288\n",
      "Iteration 18, loss = 0.15285849\n",
      "Iteration 19, loss = 0.14504585\n",
      "Iteration 20, loss = 0.13229287\n",
      "Iteration 21, loss = 0.12524434\n",
      "Iteration 22, loss = 0.11657987\n",
      "Iteration 23, loss = 0.11376670\n",
      "Iteration 24, loss = 0.10375591\n",
      "Iteration 25, loss = 0.09731361\n",
      "Iteration 26, loss = 0.09148879\n",
      "Iteration 27, loss = 0.08747972\n",
      "Iteration 28, loss = 0.08199263\n",
      "Iteration 29, loss = 0.07829373\n",
      "Iteration 30, loss = 0.07392342\n",
      "Iteration 31, loss = 0.07069190\n",
      "Iteration 32, loss = 0.06714269\n",
      "Iteration 33, loss = 0.06523965\n",
      "Iteration 34, loss = 0.06370681\n",
      "Iteration 35, loss = 0.06172131\n",
      "Iteration 36, loss = 0.05744188\n",
      "Iteration 37, loss = 0.05453374\n",
      "Iteration 38, loss = 0.05369264\n",
      "Iteration 39, loss = 0.05078844\n",
      "Iteration 40, loss = 0.05248993\n",
      "Iteration 41, loss = 0.04952145\n",
      "Iteration 42, loss = 0.04995904\n",
      "Iteration 43, loss = 0.04513546\n",
      "Iteration 44, loss = 0.04207829\n",
      "Iteration 45, loss = 0.04111348\n",
      "Iteration 46, loss = 0.04153050\n",
      "Iteration 47, loss = 0.03805993\n",
      "Iteration 48, loss = 0.03689760\n",
      "Iteration 49, loss = 0.03533947\n",
      "Iteration 50, loss = 0.03443759\n",
      "Iteration 51, loss = 0.03386462\n",
      "Iteration 52, loss = 0.03146165\n",
      "Iteration 53, loss = 0.03096033\n",
      "Iteration 54, loss = 0.03116724\n",
      "Iteration 55, loss = 0.03072718\n",
      "Iteration 56, loss = 0.02863203\n",
      "Iteration 57, loss = 0.02923694\n",
      "Iteration 58, loss = 0.02749046\n",
      "Iteration 59, loss = 0.02566607\n",
      "Iteration 60, loss = 0.02493587\n",
      "Iteration 61, loss = 0.02491493\n",
      "Iteration 62, loss = 0.02487722\n",
      "Iteration 63, loss = 0.02269116\n",
      "Iteration 64, loss = 0.02274472\n",
      "Iteration 65, loss = 0.02249916\n",
      "Iteration 66, loss = 0.02214521\n",
      "Iteration 67, loss = 0.02083054\n",
      "Iteration 68, loss = 0.02090449\n",
      "Iteration 69, loss = 0.01936265\n",
      "Iteration 70, loss = 0.01883319\n",
      "Iteration 71, loss = 0.01865678\n",
      "Iteration 72, loss = 0.01845455\n",
      "Iteration 73, loss = 0.01779740\n",
      "Iteration 74, loss = 0.01725853\n",
      "Iteration 75, loss = 0.01681140\n",
      "Iteration 76, loss = 0.01658312\n",
      "Iteration 77, loss = 0.01612897\n",
      "Iteration 78, loss = 0.01545612\n",
      "Iteration 79, loss = 0.01502540\n",
      "Iteration 80, loss = 0.01511348\n",
      "Iteration 81, loss = 0.01552708\n",
      "Iteration 82, loss = 0.01383556\n",
      "Iteration 83, loss = 0.01426277\n",
      "Iteration 84, loss = 0.01334473\n",
      "Iteration 85, loss = 0.01306637\n",
      "Iteration 86, loss = 0.01248028\n",
      "Iteration 87, loss = 0.01198332\n",
      "Iteration 88, loss = 0.01173578\n",
      "Iteration 89, loss = 0.01161599\n",
      "Iteration 90, loss = 0.01113308\n",
      "Iteration 91, loss = 0.01077650\n",
      "Iteration 92, loss = 0.01053962\n",
      "Iteration 93, loss = 0.01021782\n",
      "Iteration 94, loss = 0.00991909\n",
      "Iteration 95, loss = 0.00962905\n",
      "Iteration 96, loss = 0.00944505\n",
      "Iteration 97, loss = 0.00928744\n",
      "Iteration 98, loss = 0.00903719\n",
      "Iteration 99, loss = 0.00876849\n",
      "Iteration 100, loss = 0.00877660\n",
      "Iteration 101, loss = 0.00858539\n",
      "Iteration 102, loss = 0.00837241\n",
      "Iteration 103, loss = 0.00805999\n",
      "Iteration 104, loss = 0.00797946\n",
      "Iteration 105, loss = 0.00758496\n",
      "Iteration 106, loss = 0.00740075\n",
      "Iteration 107, loss = 0.00728253\n",
      "Iteration 108, loss = 0.00710834\n",
      "Iteration 109, loss = 0.00703167\n",
      "Iteration 110, loss = 0.00679613\n",
      "Iteration 111, loss = 0.00665705\n",
      "Iteration 112, loss = 0.00664805\n",
      "Iteration 113, loss = 0.00649086\n",
      "Iteration 114, loss = 0.00651608\n",
      "Iteration 115, loss = 0.00638105\n",
      "Iteration 116, loss = 0.00591328\n",
      "Iteration 117, loss = 0.00592195\n",
      "Iteration 118, loss = 0.00589374\n",
      "Iteration 119, loss = 0.00557580\n",
      "Iteration 120, loss = 0.00554608\n",
      "Iteration 121, loss = 0.00547963\n",
      "Iteration 122, loss = 0.00525809\n",
      "Iteration 123, loss = 0.00518810\n",
      "Iteration 124, loss = 0.00524879\n",
      "Iteration 125, loss = 0.00497903\n",
      "Iteration 126, loss = 0.00491835\n",
      "Iteration 127, loss = 0.00485018\n",
      "Iteration 128, loss = 0.00474026\n",
      "Iteration 129, loss = 0.00459102\n",
      "Iteration 130, loss = 0.00453269\n",
      "Iteration 131, loss = 0.00448512\n",
      "Iteration 132, loss = 0.00452161\n",
      "Iteration 133, loss = 0.00461853\n",
      "Iteration 134, loss = 0.00449352\n",
      "Iteration 135, loss = 0.00423903\n",
      "Iteration 136, loss = 0.00419224\n",
      "Iteration 137, loss = 0.00397205\n",
      "Iteration 138, loss = 0.00395973\n",
      "Iteration 139, loss = 0.00391470\n",
      "Iteration 140, loss = 0.00380766\n",
      "Iteration 141, loss = 0.00375868\n",
      "Iteration 142, loss = 0.00378451\n",
      "Iteration 143, loss = 0.00364980\n",
      "Iteration 144, loss = 0.00348831\n",
      "Iteration 145, loss = 0.00354732\n",
      "Iteration 146, loss = 0.00346829\n",
      "Iteration 147, loss = 0.00339675\n",
      "Iteration 148, loss = 0.00336091\n",
      "Iteration 149, loss = 0.00326047\n",
      "Iteration 150, loss = 0.00321567\n",
      "Iteration 151, loss = 0.00313726\n",
      "Iteration 152, loss = 0.00308921\n",
      "Iteration 153, loss = 0.00307068\n",
      "Iteration 154, loss = 0.00299082\n",
      "Iteration 155, loss = 0.00302749\n",
      "Iteration 156, loss = 0.00290216\n",
      "Iteration 157, loss = 0.00288899\n",
      "Iteration 158, loss = 0.00284770\n",
      "Iteration 159, loss = 0.00278148\n",
      "Iteration 160, loss = 0.00273983\n",
      "Iteration 161, loss = 0.00276275\n",
      "Iteration 162, loss = 0.00276078\n",
      "Iteration 163, loss = 0.00262834\n",
      "Iteration 164, loss = 0.00258263\n",
      "Iteration 165, loss = 0.00256737\n",
      "Iteration 166, loss = 0.00255198\n",
      "Iteration 167, loss = 0.00247432\n",
      "Iteration 168, loss = 0.00246940\n",
      "Iteration 169, loss = 0.00243677\n",
      "Iteration 170, loss = 0.00237040\n",
      "Iteration 171, loss = 0.00236676\n",
      "Iteration 172, loss = 0.00236468\n",
      "Iteration 173, loss = 0.00228975\n",
      "Iteration 174, loss = 0.00225730\n",
      "Iteration 175, loss = 0.00221999\n",
      "Iteration 176, loss = 0.00219555\n",
      "Iteration 177, loss = 0.00215870\n",
      "Iteration 178, loss = 0.00215810\n",
      "Iteration 179, loss = 0.00213799\n",
      "Iteration 180, loss = 0.00209850\n",
      "Iteration 181, loss = 0.00216340\n",
      "Iteration 182, loss = 0.00213308\n",
      "Iteration 183, loss = 0.00206069\n",
      "Iteration 184, loss = 0.00204559\n",
      "Iteration 185, loss = 0.00199274\n",
      "Iteration 186, loss = 0.00199713\n",
      "Iteration 187, loss = 0.00195688\n",
      "Iteration 188, loss = 0.00192637\n",
      "Iteration 189, loss = 0.00191676\n",
      "Iteration 190, loss = 0.00190124\n",
      "Iteration 191, loss = 0.00182490\n",
      "Iteration 192, loss = 0.00180494\n",
      "Iteration 193, loss = 0.00179137\n",
      "Iteration 194, loss = 0.00176345\n",
      "Iteration 195, loss = 0.00178013\n",
      "Iteration 196, loss = 0.00172219\n",
      "Iteration 197, loss = 0.00169872\n",
      "Iteration 198, loss = 0.00169210\n",
      "Iteration 199, loss = 0.00166635\n",
      "Iteration 200, loss = 0.00164479\n",
      "Iteration 201, loss = 0.00165265\n",
      "Iteration 202, loss = 0.00159994\n",
      "Iteration 203, loss = 0.00159602\n",
      "Iteration 204, loss = 0.00155732\n",
      "Iteration 205, loss = 0.00155539\n",
      "Iteration 206, loss = 0.00154461\n",
      "Iteration 207, loss = 0.00151147\n",
      "Iteration 208, loss = 0.00152669\n",
      "Iteration 209, loss = 0.00154367\n",
      "Iteration 210, loss = 0.00151411\n",
      "Iteration 211, loss = 0.00146300\n",
      "Iteration 212, loss = 0.00146054\n",
      "Iteration 213, loss = 0.00143652\n",
      "Iteration 214, loss = 0.00140665\n",
      "Iteration 215, loss = 0.00142561\n",
      "Iteration 216, loss = 0.00138322\n",
      "Iteration 217, loss = 0.00137704\n",
      "Iteration 218, loss = 0.00135689\n",
      "Iteration 219, loss = 0.00133574\n",
      "Iteration 220, loss = 0.00132564\n",
      "Iteration 221, loss = 0.00131150\n",
      "Iteration 222, loss = 0.00129974\n",
      "Iteration 223, loss = 0.00128756\n",
      "Iteration 224, loss = 0.00127627\n",
      "Iteration 225, loss = 0.00125729\n",
      "Iteration 226, loss = 0.00127321\n",
      "Iteration 227, loss = 0.00125724\n",
      "Iteration 228, loss = 0.00125232\n",
      "Iteration 229, loss = 0.00121992\n",
      "Iteration 230, loss = 0.00121150\n",
      "Iteration 231, loss = 0.00121483\n",
      "Iteration 232, loss = 0.00117516\n",
      "Iteration 233, loss = 0.00119528\n",
      "Iteration 234, loss = 0.00118694\n",
      "Iteration 235, loss = 0.00115337\n",
      "Iteration 236, loss = 0.00114198\n",
      "Iteration 237, loss = 0.00112846\n",
      "Iteration 238, loss = 0.00113812\n",
      "Iteration 239, loss = 0.00110716\n",
      "Iteration 240, loss = 0.00109737\n",
      "Iteration 241, loss = 0.00112661\n",
      "Iteration 242, loss = 0.00107369\n",
      "Iteration 243, loss = 0.00107207\n",
      "Iteration 244, loss = 0.00105864\n",
      "Iteration 245, loss = 0.00105031\n",
      "Iteration 246, loss = 0.00104186\n",
      "Iteration 247, loss = 0.00103423\n",
      "Iteration 248, loss = 0.00103144\n",
      "Iteration 249, loss = 0.00101698\n",
      "Iteration 250, loss = 0.00100205\n",
      "Iteration 251, loss = 0.00100096\n",
      "Iteration 252, loss = 0.00099298\n",
      "Iteration 253, loss = 0.00099033\n",
      "Iteration 254, loss = 0.00097258\n",
      "Iteration 255, loss = 0.00096797\n",
      "Iteration 256, loss = 0.00095127\n",
      "Iteration 257, loss = 0.00094649\n",
      "Iteration 258, loss = 0.00094098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 259, loss = 0.00093227\n",
      "Iteration 260, loss = 0.00092246\n",
      "Iteration 261, loss = 0.00092273\n",
      "Iteration 262, loss = 0.00091327\n",
      "Iteration 263, loss = 0.00092032\n",
      "Iteration 264, loss = 0.00091928\n",
      "Iteration 265, loss = 0.00089420\n",
      "Iteration 266, loss = 0.00088823\n",
      "Iteration 267, loss = 0.00087045\n",
      "Iteration 268, loss = 0.00088424\n",
      "Iteration 269, loss = 0.00087392\n",
      "Iteration 270, loss = 0.00085681\n",
      "Iteration 271, loss = 0.00085411\n",
      "Iteration 272, loss = 0.00084025\n",
      "Iteration 273, loss = 0.00083652\n",
      "Iteration 274, loss = 0.00082346\n",
      "Iteration 275, loss = 0.00081967\n",
      "Iteration 276, loss = 0.00081330\n",
      "Iteration 277, loss = 0.00081187\n",
      "Iteration 278, loss = 0.00079811\n",
      "Iteration 279, loss = 0.00079704\n",
      "Iteration 280, loss = 0.00078728\n",
      "Iteration 281, loss = 0.00078329\n",
      "Iteration 282, loss = 0.00078072\n",
      "Iteration 283, loss = 0.00078191\n",
      "Iteration 284, loss = 0.00076178\n",
      "Iteration 285, loss = 0.00076272\n",
      "Iteration 286, loss = 0.00075207\n",
      "Iteration 287, loss = 0.00074689\n",
      "Iteration 288, loss = 0.00074841\n",
      "Iteration 289, loss = 0.00073911\n",
      "Iteration 290, loss = 0.00073548\n",
      "Iteration 291, loss = 0.00072954\n",
      "Iteration 292, loss = 0.00072178\n",
      "Iteration 293, loss = 0.00072346\n",
      "Iteration 294, loss = 0.00071350\n",
      "Iteration 295, loss = 0.00070849\n",
      "Iteration 296, loss = 0.00070069\n",
      "Iteration 297, loss = 0.00069878\n",
      "Iteration 298, loss = 0.00069044\n",
      "Iteration 299, loss = 0.00069511\n",
      "Iteration 300, loss = 0.00067756\n",
      "Iteration 301, loss = 0.00068147\n",
      "Iteration 302, loss = 0.00066969\n",
      "Iteration 303, loss = 0.00068160\n",
      "Iteration 304, loss = 0.00066565\n",
      "Iteration 305, loss = 0.00066756\n",
      "Iteration 306, loss = 0.00065353\n",
      "Iteration 307, loss = 0.00065269\n",
      "Iteration 308, loss = 0.00064830\n",
      "Iteration 309, loss = 0.00064166\n",
      "Iteration 310, loss = 0.00063740\n",
      "Iteration 311, loss = 0.00063251\n",
      "Iteration 312, loss = 0.00063036\n",
      "Iteration 313, loss = 0.00064195\n",
      "Iteration 314, loss = 0.00062233\n",
      "Iteration 315, loss = 0.00062358\n",
      "Iteration 316, loss = 0.00060920\n",
      "Iteration 317, loss = 0.00061236\n",
      "Iteration 318, loss = 0.00061099\n",
      "Iteration 319, loss = 0.00060315\n",
      "Iteration 320, loss = 0.00059852\n",
      "Iteration 321, loss = 0.00060040\n",
      "Iteration 322, loss = 0.00059047\n",
      "Iteration 323, loss = 0.00059181\n",
      "Iteration 324, loss = 0.00058304\n",
      "Iteration 325, loss = 0.00058370\n",
      "Iteration 326, loss = 0.00057332\n",
      "Iteration 327, loss = 0.00057430\n",
      "Iteration 328, loss = 0.00056744\n",
      "Iteration 329, loss = 0.00056460\n",
      "Iteration 330, loss = 0.00056216\n",
      "Iteration 331, loss = 0.00055790\n",
      "Iteration 332, loss = 0.00055663\n",
      "Iteration 333, loss = 0.00055044\n",
      "Iteration 334, loss = 0.00054749\n",
      "Iteration 335, loss = 0.00054404\n",
      "Iteration 336, loss = 0.00054250\n",
      "Iteration 337, loss = 0.00053758\n",
      "Iteration 338, loss = 0.00053555\n",
      "Iteration 339, loss = 0.00053114\n",
      "Iteration 340, loss = 0.00052780\n",
      "Iteration 341, loss = 0.00052527\n",
      "Iteration 342, loss = 0.00052244\n",
      "Iteration 343, loss = 0.00051827\n",
      "Iteration 344, loss = 0.00051870\n",
      "Iteration 345, loss = 0.00052236\n",
      "Iteration 346, loss = 0.00051256\n",
      "Iteration 347, loss = 0.00050710\n",
      "Iteration 348, loss = 0.00050667\n",
      "Iteration 349, loss = 0.00050078\n",
      "Iteration 350, loss = 0.00049880\n",
      "Iteration 351, loss = 0.00049494\n",
      "Iteration 352, loss = 0.00049300\n",
      "Iteration 353, loss = 0.00048985\n",
      "Iteration 354, loss = 0.00049325\n",
      "Iteration 355, loss = 0.00048595\n",
      "Iteration 356, loss = 0.00048290\n",
      "Iteration 357, loss = 0.00047963\n",
      "Iteration 358, loss = 0.00047839\n",
      "Iteration 359, loss = 0.00047366\n",
      "Iteration 360, loss = 0.00047173\n",
      "Iteration 361, loss = 0.00047140\n",
      "Iteration 362, loss = 0.00046661\n",
      "Iteration 363, loss = 0.00046563\n",
      "Iteration 364, loss = 0.00046143\n",
      "Iteration 365, loss = 0.00046133\n",
      "Iteration 366, loss = 0.00045705\n",
      "Iteration 367, loss = 0.00045471\n",
      "Iteration 368, loss = 0.00045308\n",
      "Iteration 369, loss = 0.00045062\n",
      "Iteration 370, loss = 0.00044870\n",
      "Iteration 371, loss = 0.00044384\n",
      "Iteration 372, loss = 0.00044666\n",
      "Iteration 373, loss = 0.00043929\n",
      "Iteration 374, loss = 0.00044477\n",
      "Iteration 375, loss = 0.00043604\n",
      "Iteration 376, loss = 0.00043969\n",
      "Iteration 377, loss = 0.00043318\n",
      "Iteration 378, loss = 0.00043043\n",
      "Iteration 379, loss = 0.00042705\n",
      "Iteration 380, loss = 0.00042697\n",
      "Iteration 381, loss = 0.00042211\n",
      "Iteration 382, loss = 0.00042161\n",
      "Iteration 383, loss = 0.00041794\n",
      "Iteration 384, loss = 0.00041622\n",
      "Iteration 385, loss = 0.00041375\n",
      "Iteration 386, loss = 0.00041480\n",
      "Iteration 387, loss = 0.00040978\n",
      "Iteration 388, loss = 0.00040803\n",
      "Iteration 389, loss = 0.00040677\n",
      "Iteration 390, loss = 0.00040454\n",
      "Iteration 391, loss = 0.00040226\n",
      "Iteration 392, loss = 0.00040242\n",
      "Iteration 393, loss = 0.00039846\n",
      "Iteration 394, loss = 0.00039885\n",
      "Iteration 395, loss = 0.00039567\n",
      "Iteration 396, loss = 0.00039170\n",
      "Iteration 397, loss = 0.00039082\n",
      "Iteration 398, loss = 0.00038905\n",
      "Iteration 399, loss = 0.00038697\n",
      "Iteration 400, loss = 0.00038881\n",
      "Iteration 401, loss = 0.00038363\n",
      "Iteration 402, loss = 0.00038614\n",
      "Iteration 403, loss = 0.00037972\n",
      "Iteration 404, loss = 0.00037936\n",
      "Iteration 405, loss = 0.00037683\n",
      "Iteration 406, loss = 0.00037426\n",
      "Iteration 407, loss = 0.00037195\n",
      "Iteration 408, loss = 0.00037056\n",
      "Iteration 409, loss = 0.00036939\n",
      "Iteration 410, loss = 0.00036752\n",
      "Iteration 411, loss = 0.00036516\n",
      "Iteration 412, loss = 0.00036464\n",
      "Iteration 413, loss = 0.00036206\n",
      "Iteration 414, loss = 0.00036071\n",
      "Iteration 415, loss = 0.00035933\n",
      "Iteration 416, loss = 0.00035894\n",
      "Iteration 417, loss = 0.00035687\n",
      "Iteration 418, loss = 0.00035543\n",
      "Iteration 419, loss = 0.00035601\n",
      "Iteration 420, loss = 0.00035651\n",
      "Iteration 421, loss = 0.00034957\n",
      "Iteration 422, loss = 0.00034945\n",
      "Iteration 423, loss = 0.00034743\n",
      "Iteration 424, loss = 0.00034504\n",
      "Iteration 425, loss = 0.00034400\n",
      "Iteration 426, loss = 0.00034321\n",
      "Iteration 427, loss = 0.00034162\n",
      "Iteration 428, loss = 0.00033919\n",
      "Iteration 429, loss = 0.00033765\n",
      "Iteration 430, loss = 0.00033817\n",
      "Iteration 431, loss = 0.00033618\n",
      "Iteration 432, loss = 0.00033387\n",
      "Iteration 433, loss = 0.00033429\n",
      "Iteration 434, loss = 0.00033233\n",
      "Iteration 435, loss = 0.00033031\n",
      "Iteration 436, loss = 0.00032786\n",
      "Iteration 437, loss = 0.00032923\n",
      "Iteration 438, loss = 0.00032552\n",
      "Iteration 439, loss = 0.00032512\n",
      "Iteration 440, loss = 0.00032288\n",
      "Iteration 441, loss = 0.00032098\n",
      "Iteration 442, loss = 0.00031979\n",
      "Iteration 443, loss = 0.00031864\n",
      "Iteration 444, loss = 0.00031759\n",
      "Iteration 445, loss = 0.00031578\n",
      "Iteration 446, loss = 0.00031485\n",
      "Iteration 447, loss = 0.00031338\n",
      "Iteration 448, loss = 0.00031458\n",
      "Iteration 449, loss = 0.00031448\n",
      "Iteration 450, loss = 0.00031082\n",
      "Iteration 451, loss = 0.00030828\n",
      "Iteration 452, loss = 0.00030914\n",
      "Iteration 453, loss = 0.00030663\n",
      "Iteration 454, loss = 0.00030489\n",
      "Iteration 455, loss = 0.00030360\n",
      "Iteration 456, loss = 0.00030259\n",
      "Iteration 457, loss = 0.00030217\n",
      "Iteration 458, loss = 0.00030074\n",
      "Iteration 459, loss = 0.00029875\n",
      "Iteration 460, loss = 0.00029803\n",
      "Iteration 461, loss = 0.00029725\n",
      "Iteration 462, loss = 0.00029531\n",
      "Iteration 463, loss = 0.00029457\n",
      "Iteration 464, loss = 0.00029395\n",
      "Iteration 465, loss = 0.00029240\n",
      "Iteration 466, loss = 0.00029144\n",
      "Iteration 467, loss = 0.00029009\n",
      "Iteration 468, loss = 0.00028879\n",
      "Iteration 469, loss = 0.00028807\n",
      "Iteration 470, loss = 0.00028684\n",
      "Iteration 471, loss = 0.00028651\n",
      "Iteration 472, loss = 0.00028467\n",
      "Iteration 473, loss = 0.00028358\n",
      "Iteration 474, loss = 0.00028375\n",
      "Iteration 475, loss = 0.00028167\n",
      "Iteration 476, loss = 0.00028181\n",
      "Iteration 477, loss = 0.00027949\n",
      "Iteration 478, loss = 0.00027919\n",
      "Iteration 479, loss = 0.00027770\n",
      "Iteration 480, loss = 0.00027671\n",
      "Iteration 481, loss = 0.00027564\n",
      "Iteration 482, loss = 0.00027440\n",
      "Iteration 483, loss = 0.00027502\n",
      "Iteration 484, loss = 0.00027209\n",
      "Iteration 485, loss = 0.00027465\n",
      "Iteration 486, loss = 0.00027174\n",
      "Iteration 487, loss = 0.00026982\n",
      "Iteration 488, loss = 0.00026866\n",
      "Iteration 489, loss = 0.00026794\n",
      "Iteration 490, loss = 0.00026772\n",
      "Iteration 491, loss = 0.00026663\n",
      "Iteration 492, loss = 0.00026806\n",
      "Iteration 493, loss = 0.00026824\n",
      "Iteration 494, loss = 0.00026443\n",
      "Iteration 495, loss = 0.00026152\n",
      "Iteration 496, loss = 0.00026221\n",
      "Iteration 497, loss = 0.00026177\n",
      "Iteration 498, loss = 0.00025957\n",
      "Iteration 499, loss = 0.00025872\n",
      "Iteration 500, loss = 0.00025803\n",
      "Iteration 501, loss = 0.00025716\n",
      "Iteration 502, loss = 0.00025586\n",
      "Iteration 503, loss = 0.00025556\n",
      "Iteration 504, loss = 0.00025461\n",
      "Iteration 505, loss = 0.00025422\n",
      "Iteration 506, loss = 0.00025260\n",
      "Iteration 507, loss = 0.00025206\n",
      "Iteration 508, loss = 0.00025046\n",
      "Iteration 509, loss = 0.00025018\n",
      "Iteration 510, loss = 0.00024988\n",
      "Iteration 511, loss = 0.00024906\n",
      "Iteration 512, loss = 0.00025070\n",
      "Iteration 513, loss = 0.00024935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 514, loss = 0.00024787\n",
      "Iteration 515, loss = 0.00024483\n",
      "Iteration 516, loss = 0.00024492\n",
      "Iteration 517, loss = 0.00024440\n",
      "Iteration 518, loss = 0.00024298\n",
      "Iteration 519, loss = 0.00024214\n",
      "Iteration 520, loss = 0.00024149\n",
      "Iteration 521, loss = 0.00024087\n",
      "Iteration 522, loss = 0.00024004\n",
      "Iteration 523, loss = 0.00023933\n",
      "Iteration 524, loss = 0.00023886\n",
      "Iteration 525, loss = 0.00023766\n",
      "Iteration 526, loss = 0.00023701\n",
      "Iteration 527, loss = 0.00023622\n",
      "Iteration 528, loss = 0.00023666\n",
      "Iteration 529, loss = 0.00023559\n",
      "Iteration 530, loss = 0.00023440\n",
      "Iteration 531, loss = 0.00023325\n",
      "Iteration 532, loss = 0.00023461\n",
      "Iteration 533, loss = 0.00023199\n",
      "Iteration 534, loss = 0.00023251\n",
      "Iteration 535, loss = 0.00023051\n",
      "Iteration 536, loss = 0.00022983\n",
      "Iteration 537, loss = 0.00022915\n",
      "Iteration 538, loss = 0.00022831\n",
      "Iteration 539, loss = 0.00022812\n",
      "Iteration 540, loss = 0.00022685\n",
      "Iteration 541, loss = 0.00022673\n",
      "Iteration 542, loss = 0.00022615\n",
      "Iteration 543, loss = 0.00022590\n",
      "Iteration 544, loss = 0.00022530\n",
      "Iteration 545, loss = 0.00022383\n",
      "Iteration 546, loss = 0.00022286\n",
      "Iteration 547, loss = 0.00022241\n",
      "Iteration 548, loss = 0.00022228\n",
      "Iteration 549, loss = 0.00022286\n",
      "Iteration 550, loss = 0.00022220\n",
      "Iteration 551, loss = 0.00021995\n",
      "Iteration 552, loss = 0.00021909\n",
      "Iteration 553, loss = 0.00021859\n",
      "Iteration 554, loss = 0.00021839\n",
      "Iteration 555, loss = 0.00021878\n",
      "Iteration 556, loss = 0.00021691\n",
      "Iteration 557, loss = 0.00021588\n",
      "Iteration 558, loss = 0.00021687\n",
      "Iteration 559, loss = 0.00021474\n",
      "Iteration 560, loss = 0.00021511\n",
      "Iteration 561, loss = 0.00021367\n",
      "Iteration 562, loss = 0.00021319\n",
      "Iteration 563, loss = 0.00021266\n",
      "Iteration 564, loss = 0.00021405\n",
      "Iteration 565, loss = 0.00021116\n",
      "Iteration 566, loss = 0.00021092\n",
      "Iteration 567, loss = 0.00021110\n",
      "Iteration 568, loss = 0.00020961\n",
      "Iteration 569, loss = 0.00020924\n",
      "Iteration 570, loss = 0.00020875\n",
      "Iteration 571, loss = 0.00020792\n",
      "Iteration 572, loss = 0.00020764\n",
      "Iteration 573, loss = 0.00020673\n",
      "Iteration 574, loss = 0.00020636\n",
      "Iteration 575, loss = 0.00020598\n",
      "Iteration 576, loss = 0.00020540\n",
      "Iteration 577, loss = 0.00020487\n",
      "Iteration 578, loss = 0.00020408\n",
      "Iteration 579, loss = 0.00020374\n",
      "Iteration 580, loss = 0.00020305\n",
      "Iteration 581, loss = 0.00020310\n",
      "Iteration 582, loss = 0.00020219\n",
      "Iteration 583, loss = 0.00020187\n",
      "Iteration 584, loss = 0.00020099\n",
      "Iteration 585, loss = 0.00020110\n",
      "Iteration 586, loss = 0.00019965\n",
      "Iteration 587, loss = 0.00019947\n",
      "Iteration 588, loss = 0.00020049\n",
      "Iteration 589, loss = 0.00019872\n",
      "Iteration 590, loss = 0.00019853\n",
      "Iteration 591, loss = 0.00019730\n",
      "Iteration 592, loss = 0.00019690\n",
      "Iteration 593, loss = 0.00019653\n",
      "Iteration 594, loss = 0.00019629\n",
      "Iteration 595, loss = 0.00019537\n",
      "Iteration 596, loss = 0.00019531\n",
      "Iteration 597, loss = 0.00019430\n",
      "Iteration 598, loss = 0.00019442\n",
      "Iteration 599, loss = 0.00019409\n",
      "Iteration 600, loss = 0.00019398\n",
      "Iteration 601, loss = 0.00019324\n",
      "Iteration 602, loss = 0.00019285\n",
      "Iteration 603, loss = 0.00019170\n",
      "Iteration 604, loss = 0.00019149\n",
      "Iteration 605, loss = 0.00019058\n",
      "Iteration 606, loss = 0.00019024\n",
      "Iteration 607, loss = 0.00019154\n",
      "Iteration 608, loss = 0.00018936\n",
      "Iteration 609, loss = 0.00018948\n",
      "Iteration 610, loss = 0.00018872\n",
      "Iteration 611, loss = 0.00018811\n",
      "Iteration 612, loss = 0.00018767\n",
      "Iteration 613, loss = 0.00018763\n",
      "Iteration 614, loss = 0.00018731\n",
      "Iteration 615, loss = 0.00018617\n",
      "Iteration 616, loss = 0.00018627\n",
      "Iteration 617, loss = 0.00018544\n",
      "Iteration 618, loss = 0.00018499\n",
      "Iteration 619, loss = 0.00018470\n",
      "Iteration 620, loss = 0.00018394\n",
      "Iteration 621, loss = 0.00018352\n",
      "Iteration 622, loss = 0.00018326\n",
      "Iteration 623, loss = 0.00018287\n",
      "Iteration 624, loss = 0.00018248\n",
      "Iteration 625, loss = 0.00018199\n",
      "Iteration 626, loss = 0.00018159\n",
      "Iteration 627, loss = 0.00018114\n",
      "Iteration 628, loss = 0.00018084\n",
      "Iteration 629, loss = 0.00018034\n",
      "Iteration 630, loss = 0.00017996\n",
      "Iteration 631, loss = 0.00017950\n",
      "Iteration 632, loss = 0.00017912\n",
      "Iteration 633, loss = 0.00017879\n",
      "Iteration 634, loss = 0.00017836\n",
      "Iteration 635, loss = 0.00017797\n",
      "Iteration 636, loss = 0.00017747\n",
      "Iteration 637, loss = 0.00017727\n",
      "Iteration 638, loss = 0.00017680\n",
      "Iteration 639, loss = 0.00017645\n",
      "Iteration 640, loss = 0.00017606\n",
      "Iteration 641, loss = 0.00017557\n",
      "Iteration 642, loss = 0.00017551\n",
      "Iteration 643, loss = 0.00017490\n",
      "Iteration 644, loss = 0.00017467\n",
      "Iteration 645, loss = 0.00017437\n",
      "Iteration 646, loss = 0.00017460\n",
      "Iteration 647, loss = 0.00017357\n",
      "Iteration 648, loss = 0.00017311\n",
      "Iteration 649, loss = 0.00017302\n",
      "Iteration 650, loss = 0.00017246\n",
      "Iteration 651, loss = 0.00017201\n",
      "Iteration 652, loss = 0.00017174\n",
      "Iteration 653, loss = 0.00017126\n",
      "Iteration 654, loss = 0.00017144\n",
      "Iteration 655, loss = 0.00017037\n",
      "Iteration 656, loss = 0.00017024\n",
      "Iteration 657, loss = 0.00016977\n",
      "Iteration 658, loss = 0.00016941\n",
      "Iteration 659, loss = 0.00016918\n",
      "Iteration 660, loss = 0.00016904\n",
      "Iteration 661, loss = 0.00016865\n",
      "Iteration 662, loss = 0.00016836\n",
      "Iteration 663, loss = 0.00016782\n",
      "Iteration 664, loss = 0.00016728\n",
      "Iteration 665, loss = 0.00016724\n",
      "Iteration 666, loss = 0.00016743\n",
      "Iteration 667, loss = 0.00016652\n",
      "Iteration 668, loss = 0.00016613\n",
      "Iteration 669, loss = 0.00016598\n",
      "Iteration 670, loss = 0.00016570\n",
      "Iteration 671, loss = 0.00016539\n",
      "Iteration 672, loss = 0.00016467\n",
      "Iteration 673, loss = 0.00016456\n",
      "Iteration 674, loss = 0.00016439\n",
      "Iteration 675, loss = 0.00016393\n",
      "Iteration 676, loss = 0.00016372\n",
      "Iteration 677, loss = 0.00016341\n",
      "Iteration 678, loss = 0.00016340\n",
      "Iteration 679, loss = 0.00016250\n",
      "Iteration 680, loss = 0.00016290\n",
      "Iteration 681, loss = 0.00016287\n",
      "Iteration 682, loss = 0.00016252\n",
      "Iteration 683, loss = 0.00016215\n",
      "Iteration 684, loss = 0.00016136\n",
      "Iteration 685, loss = 0.00016079\n",
      "Iteration 686, loss = 0.00016086\n",
      "Iteration 687, loss = 0.00016123\n",
      "Iteration 688, loss = 0.00015986\n",
      "Iteration 689, loss = 0.00015976\n",
      "Iteration 690, loss = 0.00015939\n",
      "Iteration 691, loss = 0.00015889\n",
      "Iteration 692, loss = 0.00015909\n",
      "Iteration 693, loss = 0.00015884\n",
      "Iteration 694, loss = 0.00015852\n",
      "Iteration 695, loss = 0.00015785\n",
      "Iteration 696, loss = 0.00015740\n",
      "Iteration 697, loss = 0.00015730\n",
      "Iteration 698, loss = 0.00015706\n",
      "Iteration 699, loss = 0.00015692\n",
      "Iteration 700, loss = 0.00015655\n",
      "Iteration 701, loss = 0.00015608\n",
      "Iteration 702, loss = 0.00015579\n",
      "Iteration 703, loss = 0.00015667\n",
      "Iteration 704, loss = 0.00015593\n",
      "Iteration 705, loss = 0.00015528\n",
      "Iteration 706, loss = 0.00015475\n",
      "Iteration 707, loss = 0.00015456\n",
      "Iteration 708, loss = 0.00015412\n",
      "Iteration 709, loss = 0.00015401\n",
      "Iteration 710, loss = 0.00015368\n",
      "Iteration 711, loss = 0.00015329\n",
      "Iteration 712, loss = 0.00015332\n",
      "Iteration 713, loss = 0.00015277\n",
      "Iteration 714, loss = 0.00015264\n",
      "Iteration 715, loss = 0.00015238\n",
      "Iteration 716, loss = 0.00015194\n",
      "Iteration 717, loss = 0.00015174\n",
      "Iteration 718, loss = 0.00015166\n",
      "Iteration 719, loss = 0.00015131\n",
      "Iteration 720, loss = 0.00015164\n",
      "Iteration 721, loss = 0.00015080\n",
      "Iteration 722, loss = 0.00015068\n",
      "Iteration 723, loss = 0.00015036\n",
      "Iteration 724, loss = 0.00014988\n",
      "Iteration 725, loss = 0.00014972\n",
      "Iteration 726, loss = 0.00014952\n",
      "Iteration 727, loss = 0.00014936\n",
      "Iteration 728, loss = 0.00014902\n",
      "Iteration 729, loss = 0.00014874\n",
      "Iteration 730, loss = 0.00014909\n",
      "Iteration 731, loss = 0.00014828\n",
      "Iteration 732, loss = 0.00014826\n",
      "Iteration 733, loss = 0.00014786\n",
      "Iteration 734, loss = 0.00014771\n",
      "Iteration 735, loss = 0.00014757\n",
      "Iteration 736, loss = 0.00014726\n",
      "Iteration 737, loss = 0.00014725\n",
      "Iteration 738, loss = 0.00014689\n",
      "Iteration 739, loss = 0.00014655\n",
      "Iteration 740, loss = 0.00014611\n",
      "Iteration 741, loss = 0.00014593\n",
      "Iteration 742, loss = 0.00014583\n",
      "Iteration 743, loss = 0.00014538\n",
      "Iteration 744, loss = 0.00014515\n",
      "Iteration 745, loss = 0.00014497\n",
      "Iteration 746, loss = 0.00014485\n",
      "Iteration 747, loss = 0.00014492\n",
      "Iteration 748, loss = 0.00014452\n",
      "Iteration 749, loss = 0.00014495\n",
      "Iteration 750, loss = 0.00014384\n",
      "Iteration 751, loss = 0.00014392\n",
      "Iteration 752, loss = 0.00014329\n",
      "Iteration 753, loss = 0.00014326\n",
      "Iteration 754, loss = 0.00014303\n",
      "Iteration 755, loss = 0.00014332\n",
      "Iteration 756, loss = 0.00014350\n",
      "Iteration 757, loss = 0.00014267\n",
      "Iteration 758, loss = 0.00014209\n",
      "Iteration 759, loss = 0.00014230\n",
      "Iteration 760, loss = 0.00014176\n",
      "Iteration 761, loss = 0.00014144\n",
      "Iteration 762, loss = 0.00014121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 763, loss = 0.00014105\n",
      "Iteration 764, loss = 0.00014130\n",
      "Iteration 765, loss = 0.00014071\n",
      "Iteration 766, loss = 0.00014162\n",
      "Iteration 767, loss = 0.00014014\n",
      "Iteration 768, loss = 0.00014059\n",
      "Iteration 769, loss = 0.00014027\n",
      "Iteration 770, loss = 0.00013985\n",
      "Iteration 771, loss = 0.00013941\n",
      "Iteration 772, loss = 0.00014046\n",
      "Iteration 773, loss = 0.00013910\n",
      "Iteration 774, loss = 0.00013904\n",
      "Iteration 775, loss = 0.00013898\n",
      "Iteration 776, loss = 0.00013836\n",
      "Iteration 777, loss = 0.00013821\n",
      "Iteration 778, loss = 0.00013850\n",
      "Iteration 779, loss = 0.00013856\n",
      "Iteration 780, loss = 0.00013792\n",
      "Iteration 781, loss = 0.00013736\n",
      "Iteration 782, loss = 0.00013745\n",
      "Iteration 783, loss = 0.00013768\n",
      "Iteration 784, loss = 0.00013682\n",
      "Iteration 785, loss = 0.00013700\n",
      "Iteration 786, loss = 0.00013667\n",
      "Iteration 787, loss = 0.00013729\n",
      "Iteration 788, loss = 0.00013645\n",
      "Iteration 789, loss = 0.00013606\n",
      "Iteration 790, loss = 0.00013580\n",
      "Iteration 791, loss = 0.00013553\n",
      "Iteration 792, loss = 0.00013609\n",
      "Iteration 793, loss = 0.00013507\n",
      "Iteration 794, loss = 0.00013536\n",
      "Iteration 795, loss = 0.00013511\n",
      "Iteration 796, loss = 0.00013496\n",
      "Iteration 797, loss = 0.00013459\n",
      "Iteration 798, loss = 0.00013430\n",
      "Iteration 799, loss = 0.00013445\n",
      "Iteration 800, loss = 0.00013407\n",
      "Iteration 801, loss = 0.00013373\n",
      "Iteration 802, loss = 0.00013365\n",
      "Iteration 803, loss = 0.00013343\n",
      "Iteration 804, loss = 0.00013323\n",
      "Iteration 805, loss = 0.00013348\n",
      "Iteration 806, loss = 0.00013292\n",
      "Iteration 807, loss = 0.00013280\n",
      "Iteration 808, loss = 0.00013268\n",
      "Iteration 809, loss = 0.00013234\n",
      "Iteration 810, loss = 0.00013224\n",
      "Iteration 811, loss = 0.00013203\n",
      "Iteration 812, loss = 0.00013196\n",
      "Iteration 813, loss = 0.00013179\n",
      "Iteration 814, loss = 0.00013165\n",
      "Iteration 815, loss = 0.00013138\n",
      "Iteration 816, loss = 0.00013133\n",
      "Iteration 817, loss = 0.00013109\n",
      "Iteration 818, loss = 0.00013103\n",
      "Iteration 819, loss = 0.00013089\n",
      "Iteration 820, loss = 0.00013082\n",
      "Iteration 821, loss = 0.00013041\n",
      "Iteration 822, loss = 0.00013029\n",
      "Iteration 823, loss = 0.00013012\n",
      "Iteration 824, loss = 0.00012993\n",
      "Iteration 825, loss = 0.00012976\n",
      "Iteration 826, loss = 0.00012958\n",
      "Iteration 827, loss = 0.00012946\n",
      "Iteration 828, loss = 0.00012986\n",
      "Iteration 829, loss = 0.00012946\n",
      "Iteration 830, loss = 0.00012906\n",
      "Iteration 831, loss = 0.00012890\n",
      "Iteration 832, loss = 0.00012868\n",
      "Iteration 833, loss = 0.00012873\n",
      "Iteration 834, loss = 0.00012827\n",
      "Iteration 835, loss = 0.00012821\n",
      "Iteration 836, loss = 0.00012811\n",
      "Iteration 837, loss = 0.00012797\n",
      "Iteration 838, loss = 0.00012769\n",
      "Iteration 839, loss = 0.00012778\n",
      "Iteration 840, loss = 0.00012747\n",
      "Iteration 841, loss = 0.00012728\n",
      "Iteration 842, loss = 0.00012726\n",
      "Iteration 843, loss = 0.00012732\n",
      "Iteration 844, loss = 0.00012698\n",
      "Iteration 845, loss = 0.00012683\n",
      "Iteration 846, loss = 0.00012658\n",
      "Iteration 847, loss = 0.00012644\n",
      "Iteration 848, loss = 0.00012632\n",
      "Iteration 849, loss = 0.00012613\n",
      "Iteration 850, loss = 0.00012595\n",
      "Iteration 851, loss = 0.00012590\n",
      "Iteration 852, loss = 0.00012567\n",
      "Iteration 853, loss = 0.00012561\n",
      "Iteration 854, loss = 0.00012540\n",
      "Iteration 855, loss = 0.00012536\n",
      "Iteration 856, loss = 0.00012518\n",
      "Iteration 857, loss = 0.00012495\n",
      "Iteration 858, loss = 0.00012517\n",
      "Iteration 859, loss = 0.00012474\n",
      "Iteration 860, loss = 0.00012468\n",
      "Iteration 861, loss = 0.00012437\n",
      "Iteration 862, loss = 0.00012430\n",
      "Iteration 863, loss = 0.00012455\n",
      "Iteration 864, loss = 0.00012404\n",
      "Iteration 865, loss = 0.00012388\n",
      "Iteration 866, loss = 0.00012418\n",
      "Iteration 867, loss = 0.00012365\n",
      "Iteration 868, loss = 0.00012351\n",
      "Iteration 869, loss = 0.00012336\n",
      "Iteration 870, loss = 0.00012331\n",
      "Iteration 871, loss = 0.00012384\n",
      "Iteration 872, loss = 0.00012315\n",
      "Iteration 873, loss = 0.00012287\n",
      "Iteration 874, loss = 0.00012276\n",
      "Iteration 875, loss = 0.00012251\n",
      "Iteration 876, loss = 0.00012241\n",
      "Iteration 877, loss = 0.00012229\n",
      "Iteration 878, loss = 0.00012214\n",
      "Iteration 879, loss = 0.00012216\n",
      "Iteration 880, loss = 0.00012187\n",
      "Iteration 881, loss = 0.00012190\n",
      "Iteration 882, loss = 0.00012167\n",
      "Iteration 883, loss = 0.00012163\n",
      "Iteration 884, loss = 0.00012160\n",
      "Iteration 885, loss = 0.00012127\n",
      "Iteration 886, loss = 0.00012112\n",
      "Iteration 887, loss = 0.00012107\n",
      "Iteration 888, loss = 0.00012096\n",
      "Iteration 889, loss = 0.00012091\n",
      "Iteration 890, loss = 0.00012073\n",
      "Iteration 891, loss = 0.00012070\n",
      "Iteration 892, loss = 0.00012043\n",
      "Iteration 893, loss = 0.00012034\n",
      "Iteration 894, loss = 0.00012071\n",
      "Iteration 895, loss = 0.00012019\n",
      "Iteration 896, loss = 0.00012006\n",
      "Iteration 897, loss = 0.00011984\n",
      "Iteration 898, loss = 0.00011975\n",
      "Iteration 899, loss = 0.00011959\n",
      "Iteration 900, loss = 0.00011946\n",
      "Iteration 901, loss = 0.00011949\n",
      "Iteration 902, loss = 0.00011950\n",
      "Iteration 903, loss = 0.00011921\n",
      "Iteration 904, loss = 0.00011908\n",
      "Iteration 905, loss = 0.00011897\n",
      "Iteration 906, loss = 0.00011878\n",
      "Iteration 907, loss = 0.00011867\n",
      "Iteration 908, loss = 0.00011853\n",
      "Iteration 909, loss = 0.00011838\n",
      "Iteration 910, loss = 0.00011833\n",
      "Iteration 911, loss = 0.00011840\n",
      "Iteration 912, loss = 0.00011815\n",
      "Iteration 913, loss = 0.00011796\n",
      "Iteration 914, loss = 0.00011796\n",
      "Iteration 915, loss = 0.00011768\n",
      "Iteration 916, loss = 0.00011764\n",
      "Iteration 917, loss = 0.00011762\n",
      "Iteration 918, loss = 0.00011743\n",
      "Iteration 919, loss = 0.00011729\n",
      "Iteration 920, loss = 0.00011722\n",
      "Iteration 921, loss = 0.00011738\n",
      "Iteration 922, loss = 0.00011707\n",
      "Iteration 923, loss = 0.00011690\n",
      "Iteration 924, loss = 0.00011690\n",
      "Iteration 925, loss = 0.00011679\n",
      "Iteration 926, loss = 0.00011669\n",
      "Iteration 927, loss = 0.00011639\n",
      "Iteration 928, loss = 0.00011655\n",
      "Iteration 929, loss = 0.00011624\n",
      "Iteration 930, loss = 0.00011624\n",
      "Iteration 931, loss = 0.00011605\n",
      "Iteration 932, loss = 0.00011587\n",
      "Iteration 933, loss = 0.00011587\n",
      "Iteration 934, loss = 0.00011583\n",
      "Iteration 935, loss = 0.00011552\n",
      "Iteration 936, loss = 0.00011548\n",
      "Iteration 937, loss = 0.00011562\n",
      "Iteration 938, loss = 0.00011525\n",
      "Iteration 939, loss = 0.00011544\n",
      "Iteration 940, loss = 0.00011510\n",
      "Iteration 941, loss = 0.00011499\n",
      "Iteration 942, loss = 0.00011486\n",
      "Iteration 943, loss = 0.00011523\n",
      "Iteration 944, loss = 0.00011492\n",
      "Iteration 945, loss = 0.00011474\n",
      "Iteration 946, loss = 0.00011450\n",
      "Iteration 947, loss = 0.00011437\n",
      "Iteration 948, loss = 0.00011430\n",
      "Iteration 949, loss = 0.00011440\n",
      "Iteration 950, loss = 0.00011406\n",
      "Iteration 951, loss = 0.00011401\n",
      "Iteration 952, loss = 0.00011389\n",
      "Iteration 953, loss = 0.00011373\n",
      "Iteration 954, loss = 0.00011373\n",
      "Iteration 955, loss = 0.00011367\n",
      "Iteration 956, loss = 0.00011345\n",
      "Iteration 957, loss = 0.00011360\n",
      "Iteration 958, loss = 0.00011343\n",
      "Iteration 959, loss = 0.00011332\n",
      "Iteration 960, loss = 0.00011339\n",
      "Iteration 961, loss = 0.00011303\n",
      "Iteration 962, loss = 0.00011288\n",
      "Iteration 963, loss = 0.00011287\n",
      "Iteration 964, loss = 0.00011271\n",
      "Iteration 965, loss = 0.00011272\n",
      "Iteration 966, loss = 0.00011266\n",
      "Iteration 967, loss = 0.00011244\n",
      "Iteration 968, loss = 0.00011235\n",
      "Iteration 969, loss = 0.00011263\n",
      "Iteration 970, loss = 0.00011225\n",
      "Iteration 971, loss = 0.00011222\n",
      "Iteration 972, loss = 0.00011211\n",
      "Iteration 973, loss = 0.00011190\n",
      "Iteration 974, loss = 0.00011182\n",
      "Iteration 975, loss = 0.00011174\n",
      "Iteration 976, loss = 0.00011165\n",
      "Iteration 977, loss = 0.00011164\n",
      "Iteration 978, loss = 0.00011153\n",
      "Iteration 979, loss = 0.00011140\n",
      "Iteration 980, loss = 0.00011134\n",
      "Iteration 981, loss = 0.00011116\n",
      "Iteration 982, loss = 0.00011104\n",
      "Iteration 983, loss = 0.00011102\n",
      "Iteration 984, loss = 0.00011103\n",
      "Iteration 985, loss = 0.00011081\n",
      "Iteration 986, loss = 0.00011076\n",
      "Iteration 987, loss = 0.00011072\n",
      "Iteration 988, loss = 0.00011054\n",
      "Iteration 989, loss = 0.00011048\n",
      "Iteration 990, loss = 0.00011044\n",
      "Iteration 991, loss = 0.00011031\n",
      "Iteration 992, loss = 0.00011043\n",
      "Iteration 993, loss = 0.00011015\n",
      "Iteration 994, loss = 0.00011012\n",
      "Iteration 995, loss = 0.00010990\n",
      "Iteration 996, loss = 0.00010999\n",
      "Iteration 997, loss = 0.00010994\n",
      "Iteration 998, loss = 0.00010994\n",
      "Iteration 999, loss = 0.00010972\n",
      "Iteration 1000, loss = 0.00010964\n"
     ]
    }
   ],
   "source": [
    "nnModelADAM = mlpADAM.fit(trainData, trainLabelE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "train_df= pd.read_csv('train.csv')\n",
    "test_df= pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data():\n",
    "    train_values = train_df.values\n",
    "    test_values = test_df.values\n",
    "    np.random.shuffle(train_values)\n",
    "    np.random.shuffle(test_values)\n",
    "    X_train = train_values[:, :-1]\n",
    "    X_test = test_values[:, :-1]\n",
    "    y_train = train_values[:, -1]\n",
    "    y_test = test_values[:, -1]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = get_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8848848848848849"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression : 88.48%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8788788788788788"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_all_data()\n",
    "pca= PCA(n_components=200)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8408408408408409"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature scaling between -1 to 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_all_data()\n",
    "scaler.fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9039039039039038"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_all_data()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest : 90.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#separating features and labels\n",
    "trainData= train.drop('Activity',axis=1).values\n",
    "trainLabel= train.Activity.values\n",
    "\n",
    "testData= test.drop('Activity', axis=1).values\n",
    "testLabel= test.Activity.values\n",
    "\n",
    "#encoding labels\n",
    "from sklearn import preprocessing\n",
    "\n",
    "encoder= preprocessing.LabelEncoder()\n",
    "\n",
    "#encoding test labels\n",
    "encoder.fit(testLabel)\n",
    "testLabelE = encoder.transform(testLabel)\n",
    "\n",
    "#encoding train labels\n",
    "encoder.fit(trainLabel)\n",
    "trainLabelE = encoder.transform(trainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy : 0.76476\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=24)\n",
    "\n",
    "knnmodel= clf.fit(trainData, trainLabelE)\n",
    "pred= clf.predict(testData)\n",
    "\n",
    "acc= accuracy_score(testLabelE, pred)\n",
    "print(\"KNN accuracy : %.5f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN accuracy : 0.76476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.757758\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "deTreeClf= DecisionTreeClassifier()\n",
    "tree = deTreeClf.fit(trainData, trainLabelE)\n",
    "testpred = tree.predict(testData)\n",
    "\n",
    "acc1= accuracy_score(testLabelE,testpred)\n",
    "print('Accuracy : %f'% acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Accuracy : 0.757758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set found:\n",
      "{'C': 100, 'kernel': 'rbf'}\n",
      "Detailed grid scores:\n",
      "0.996 (+/-0.008) for {'C': 100, 'kernel': 'linear'}\n",
      "\n",
      "0.998 (+/-0.006) for {'C': 100, 'kernel': 'rbf'}\n",
      "\n",
      "0.998 (+/-0.006) for {'C': 100, 'kernel': 'poly'}\n",
      "\n",
      "0.995 (+/-0.010) for {'C': 100, 'kernel': 'sigmoid'}\n",
      "\n",
      "0.996 (+/-0.008) for {'C': 50, 'kernel': 'linear'}\n",
      "\n",
      "0.998 (+/-0.006) for {'C': 50, 'kernel': 'rbf'}\n",
      "\n",
      "0.995 (+/-0.010) for {'C': 50, 'kernel': 'poly'}\n",
      "\n",
      "0.994 (+/-0.017) for {'C': 50, 'kernel': 'sigmoid'}\n",
      "\n",
      "0.996 (+/-0.008) for {'C': 20, 'kernel': 'linear'}\n",
      "\n",
      "0.994 (+/-0.013) for {'C': 20, 'kernel': 'rbf'}\n",
      "\n",
      "0.996 (+/-0.011) for {'C': 20, 'kernel': 'poly'}\n",
      "\n",
      "0.989 (+/-0.014) for {'C': 20, 'kernel': 'sigmoid'}\n",
      "\n",
      "0.996 (+/-0.008) for {'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "0.927 (+/-0.020) for {'C': 1, 'kernel': 'rbf'}\n",
      "\n",
      "0.657 (+/-0.023) for {'C': 1, 'kernel': 'poly'}\n",
      "\n",
      "0.806 (+/-0.015) for {'C': 1, 'kernel': 'sigmoid'}\n",
      "\n",
      "0.995 (+/-0.010) for {'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "0.389 (+/-0.003) for {'C': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "0.388 (+/-0.001) for {'C': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "0.388 (+/-0.001) for {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf', 'poly','sigmoid'],\n",
    "    'C': [100, 50, 20, 1, 0.1]\n",
    "}\n",
    "\n",
    "selector = GridSearchCV(SVC(), parameters, scoring='accuracy') # we only care about accuracy here\n",
    "selector.fit(trainData, trainLabel)\n",
    "\n",
    "print('Best parameter set found:')\n",
    "print(selector.best_params_)\n",
    "print('Detailed grid scores:')\n",
    "means = selector.cv_results_['mean_test_score']\n",
    "stds = selector.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, selector.cv_results_['params']):\n",
    "    print('%0.3f (+/-%0.03f) for %r' % (mean, std * 2, params))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8738738738738738\n"
     ]
    }
   ],
   "source": [
    "clf=SVC(kernel='linear', C=100).fit(trainData, trainLabel)\n",
    "y_pred= clf.predict(testData)\n",
    "print('Accuracy score:',accuracy_score(testLabel,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search : Accuracy score: 87.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification models\n",
    "# logistic regression : 88.48%\n",
    "#Random forest : 90.39\n",
    "#KNN accuracy : 0.76476\n",
    "# Decision Tree Accuracy : 0.757758\n",
    "#Grid search : Accuracy score: 87.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
